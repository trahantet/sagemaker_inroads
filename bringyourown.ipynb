{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import string \n",
    "import re \n",
    "import pandas as pd\n",
    "import boto3\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inroads = pd.read_csv('https://inroads-test-bucket1.s3.amazonaws.com/inroads_only.csv', dtype={'text': str})\n",
    "#df_raw = pd.read_csv('https://inroads-test-bucket1.s3.amazonaws.com/raw_data.csv', dtype={'text': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_clean = df_inroads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text): \n",
    "    return text.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    temp = text.translate(translator)\n",
    "    return re.sub('\\s+',' ', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "# lemmatize string \n",
    "\n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text)\n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return lemmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(text):\n",
    "    clean_words = []\n",
    "    for word in text:\n",
    "        new = re.sub(r'\\b\\w{,3}\\b', '', word)\n",
    "        clean_words.append(new) \n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "\n",
    "for i in range(0,len(text_to_clean)):\n",
    "    temp = str(text_to_clean.text.iloc[i])     \n",
    "    temp = text_lowercase(temp) #make all lower\n",
    "    temp_2 = remove_numbers(temp) # remove all numbers\n",
    "    temp_3 = str.strip(temp_2) # remove white spaces 1\n",
    "    temp_4 = temp_3.replace('\\n',' ').replace(\"\\t\", \" \") # remove white spaces 2\n",
    "    temp_5 = re.sub(\" +\", \" \", temp_4) # remove white spaces 3\n",
    "    temp_6 = remove_punctuation(temp_5) # remove punctuation \n",
    "    temp_7 = lemmatize_word(temp_6) # lemmatize text\n",
    "    temp_8 = [word for word in temp_7 if word.isalnum()] # remove auxiliarly punctuation\n",
    "    temp_9 = remove_short_words(temp_8) # remove short words\n",
    "    result.append(list(filter(None, temp_9))) # remove empty strings and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_clean['clean_text'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Unnamed: 0                                           resource  \\\n",
       "0              1  649607\\nresearch-article2016\\n                ...   \n",
       "1              2  649607\\nresearch-article2016\\n                ...   \n",
       "2              3  649607\\nresearch-article2016\\n                ...   \n",
       "3              4  649607\\nresearch-article2016\\n                ...   \n",
       "4              5  649607\\nresearch-article2016\\n                ...   \n",
       "...          ...                                                ...   \n",
       "1221        1222  the-inroads-team-is-growing-introducing-our-ne...   \n",
       "1222        1223  there-is-no-qualification-to-be-here-just-pass...   \n",
       "1223        1224                                           wfac.txt   \n",
       "1224        1225  what-is-inspiring-our-steering-committee-in-20...   \n",
       "1225        1226  young-people-fight-societal-norms-in-sudan-whe...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     649607\\nresearch-article2016\\n                ...   \n",
       "1     Huang et al.                                  ...   \n",
       "2     972                                           ...   \n",
       "3     Huang et al.                                  ...   \n",
       "4     974\\n      Table 1. Descriptive Statistics and...   \n",
       "...                                                 ...   \n",
       "1221  \\nSince the network launch in 2013 and the ope...   \n",
       "1222  \\nThe 2nd Bi-annual inroads Global Member Gath...   \n",
       "1223  \\nCameroon is one of seven African nations tha...   \n",
       "1224  \\nThis year brings us a group of committed ind...   \n",
       "1225  \\nWe often hear from many of our members, in d...   \n",
       "\n",
       "                                             clean_text  \n",
       "0     [research, article, pspxxx, personality, socia...  \n",
       "1     [huang, describe, current, program, research, ...  \n",
       "2     [personality, social, psychology, bulletin, mo...  \n",
       "3     [huang, abortion, attitudes, over, year, perio...  \n",
       "4     [table, descriptive, statistics, bivariate, co...  \n",
       "...                                                 ...  \n",
       "1221  [since, network, launch, open, membership, int...  \n",
       "1222  [annual, inroads, global, member, gather, take...  \n",
       "1223  [cameroon, seven, african, nations, that, upho...  \n",
       "1224  [this, year, bring, group, commit, individuals...  \n",
       "1225  [often, hear, from, many, members, digital, re...  \n",
       "\n",
       "[1226 rows x 4 columns]>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_clean.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [research, article, pspxxx, personality, socia...\n",
       "1     [huang, describe, current, program, research, ...\n",
       "2     [personality, social, psychology, bulletin, mo...\n",
       "3     [huang, abortion, attitudes, over, year, perio...\n",
       "4     [table, descriptive, statistics, bivariate, co...\n",
       "                            ...                        \n",
       "95    [towards, asps, show, figure, informants, expr...\n",
       "96    [behavior, community, towards, people, associa...\n",
       "97    [abortion, brother, stop, talk, with, after, l...\n",
       "98    [person, support, abortion, spaw, fgds, partic...\n",
       "99    [women, exclude, religious, practice, abortive...\n",
       "Name: clean_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_clean.clean_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save cleaned tokens for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inroads_clean = text_to_clean[['resource', 'clean_text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                resource  \\\n",
       "0     649607\\nresearch-article2016\\n                ...   \n",
       "1     649607\\nresearch-article2016\\n                ...   \n",
       "2     649607\\nresearch-article2016\\n                ...   \n",
       "3     649607\\nresearch-article2016\\n                ...   \n",
       "4     649607\\nresearch-article2016\\n                ...   \n",
       "...                                                 ...   \n",
       "1221  the-inroads-team-is-growing-introducing-our-ne...   \n",
       "1222  there-is-no-qualification-to-be-here-just-pass...   \n",
       "1223                                           wfac.txt   \n",
       "1224  what-is-inspiring-our-steering-committee-in-20...   \n",
       "1225  young-people-fight-societal-norms-in-sudan-whe...   \n",
       "\n",
       "                                             clean_text  \n",
       "0     [research, article, pspxxx, personality, socia...  \n",
       "1     [huang, describe, current, program, research, ...  \n",
       "2     [personality, social, psychology, bulletin, mo...  \n",
       "3     [huang, abortion, attitudes, over, year, perio...  \n",
       "4     [table, descriptive, statistics, bivariate, co...  \n",
       "...                                                 ...  \n",
       "1221  [since, network, launch, open, membership, int...  \n",
       "1222  [annual, inroads, global, member, gather, take...  \n",
       "1223  [cameroon, seven, african, nations, that, upho...  \n",
       "1224  [this, year, bring, group, commit, individuals...  \n",
       "1225  [often, hear, from, many, members, digital, re...  \n",
       "\n",
       "[1226 rows x 2 columns]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_inroads_clean.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inroads_clean.to_pickle('final_inroads_clean.pkl')\n",
    "final_inroads_clean = pd.read_pickle('final_inroads_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inroads_clean = pd.read_pickle(\"final_inroads_clean.pkl\")\n",
    "clean_inroads_tokens = []\n",
    "\n",
    "for i in range(0,len(final_inroads_clean)):\n",
    "    text = final_inroads_clean.clean_text[i]\n",
    "    #print(text)\n",
    "    clean_inroads_tokens.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of              tokens\n",
       "0          research\n",
       "1           article\n",
       "2            pspxxx\n",
       "3       personality\n",
       "4            social\n",
       "...             ...\n",
       "321033       slowly\n",
       "321034        there\n",
       "321035        space\n",
       "321036        start\n",
       "321037   discussion\n",
       "\n",
       "[321038 rows x 1 columns]>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_inroads_tokens = pd.DataFrame(clean_inroads_tokens,columns=['tokens'])\n",
    "clean_inroads_tokens.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inroads_tokens.to_pickle('clean_inroads_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inroads_tokens = pd.read_pickle('clean_inroads_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "## upload trainign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "BucketAlreadyOwnedByYou",
     "evalue": "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-4f93697291c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m s3_session.create_bucket(Bucket=bucket, \n\u001b[1;32m      6\u001b[0m                          \u001b[0mCreateBucketConfiguration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                          {'LocationConstraint': region})\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0ms3_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train/clean_full_tokens.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clean_full_tokens.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/resources/factory.py\u001b[0m in \u001b[0;36mdo_action\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;31m# instance via ``self``.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mdo_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'load'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/resources/action.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, parent, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                     operation_name, params)\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Response: %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m: An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "bucket = 'sagemaker-word2vec-scikitlearn'\n",
    "region = 'us-east-2'\n",
    "s3_session = boto3.Session().resource('s3')\n",
    "s3_session.create_bucket(Bucket=bucket, \n",
    "                         CreateBucketConfiguration=\n",
    "                         {'LocationConstraint': region})\n",
    "s3_session.Bucket(bucket).Object('train/clean_full_tokens.pkl').upload_file('clean_full_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = clean_inroads_tokens.to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {0: 'research',\n",
       "  1: 'article',\n",
       "  2: 'pspxxx',\n",
       "  3: 'personality',\n",
       "  4: 'social',\n",
       "  5: 'psychology',\n",
       "  6: 'bulletinhuang',\n",
       "  7: 'article',\n",
       "  8: 'personality',\n",
       "  9: 'social',\n",
       "  10: 'benevolent',\n",
       "  11: 'sexism',\n",
       "  12: 'attitudes',\n",
       "  13: 'toward',\n",
       "  14: 'psychology',\n",
       "  15: 'bulletin',\n",
       "  16: 'society',\n",
       "  17: 'personality',\n",
       "  18: 'motherhood',\n",
       "  19: 'reproductive',\n",
       "  20: 'right',\n",
       "  21: 'social',\n",
       "  22: 'psychology',\n",
       "  23: 'reprint',\n",
       "  24: 'permissions',\n",
       "  25: 'multi',\n",
       "  26: 'study',\n",
       "  27: 'longitudinal',\n",
       "  28: 'examination',\n",
       "  29: 'sagepub',\n",
       "  30: 'journalspermissions',\n",
       "  31: 'abortion',\n",
       "  32: 'attitudes',\n",
       "  33: 'pspb',\n",
       "  34: 'sagepub',\n",
       "  35: 'yanshu',\n",
       "  36: 'huang',\n",
       "  37: 'paul',\n",
       "  38: 'davies',\n",
       "  39: 'chris',\n",
       "  40: 'sibley',\n",
       "  41: 'danny',\n",
       "  42: 'osborne',\n",
       "  43: 'abstract',\n",
       "  44: 'although',\n",
       "  45: 'benevolent',\n",
       "  46: 'sexism',\n",
       "  47: 'ideology',\n",
       "  48: 'that',\n",
       "  49: 'highly',\n",
       "  50: 'revere',\n",
       "  51: 'women',\n",
       "  52: 'conform',\n",
       "  53: 'traditional',\n",
       "  54: 'gender',\n",
       "  55: 'cloak',\n",
       "  56: 'superficially',\n",
       "  57: 'positive',\n",
       "  58: 'tone',\n",
       "  59: 'place',\n",
       "  60: 'upon',\n",
       "  61: 'pedestal',\n",
       "  62: 'inherently',\n",
       "  63: 'restrictive',\n",
       "  64: 'accordingly',\n",
       "  65: 'because',\n",
       "  66: 'paternalistic',\n",
       "  67: 'beliefs',\n",
       "  68: 'associate',\n",
       "  69: 'with',\n",
       "  70: 'base',\n",
       "  71: 'idealization',\n",
       "  72: 'traditional',\n",
       "  73: 'gender',\n",
       "  74: 'roles',\n",
       "  75: 'which',\n",
       "  76: 'include',\n",
       "  77: 'motherhood',\n",
       "  78: 'should',\n",
       "  79: 'predict',\n",
       "  80: 'people',\n",
       "  81: 'attitudes',\n",
       "  82: 'toward',\n",
       "  83: 'women',\n",
       "  84: 'reproductive',\n",
       "  85: 'right',\n",
       "  86: 'data',\n",
       "  87: 'from',\n",
       "  88: 'nationwide',\n",
       "  89: 'longitudinal',\n",
       "  90: 'panel',\n",
       "  91: 'study',\n",
       "  92: 'study',\n",
       "  93: 'show',\n",
       "  94: 'that',\n",
       "  95: 'hostile',\n",
       "  96: 'sexism',\n",
       "  97: 'have',\n",
       "  98: 'cross',\n",
       "  99: 'effect',\n",
       "  100: 'opposition',\n",
       "  101: 'both',\n",
       "  102: 'elective',\n",
       "  103: 'traumatic',\n",
       "  104: 'abortion',\n",
       "  105: 'study',\n",
       "  106: 'extend',\n",
       "  107: 'these',\n",
       "  108: 'find',\n",
       "  109: 'show',\n",
       "  110: 'that',\n",
       "  111: 'relationship',\n",
       "  112: 'between',\n",
       "  113: 'support',\n",
       "  114: 'abortion',\n",
       "  115: 'fully',\n",
       "  116: 'mediate',\n",
       "  117: 'attitudes',\n",
       "  118: 'toward',\n",
       "  119: 'motherhood',\n",
       "  120: 'these',\n",
       "  121: 'result',\n",
       "  122: 'highlight',\n",
       "  123: 'pernicious',\n",
       "  124: 'nature',\n",
       "  125: 'demonstrate',\n",
       "  126: 'that',\n",
       "  127: 'idealization',\n",
       "  128: 'motherhood',\n",
       "  129: 'substantial',\n",
       "  130: 'cost',\n",
       "  131: 'namely',\n",
       "  132: 'restriction',\n",
       "  133: 'women',\n",
       "  134: 'reproductive',\n",
       "  135: 'right',\n",
       "  136: 'keywords',\n",
       "  137: 'benevolent',\n",
       "  138: 'sexism',\n",
       "  139: 'motherhood',\n",
       "  140: 'traumatic',\n",
       "  141: 'abortion',\n",
       "  142: 'elective',\n",
       "  143: 'abortion',\n",
       "  144: 'longitudinal',\n",
       "  145: 'receive',\n",
       "  146: 'revision',\n",
       "  147: 'accept',\n",
       "  148: 'april',\n",
       "  149: 'motherhood',\n",
       "  150: 'often',\n",
       "  151: 'regard',\n",
       "  152: 'essential',\n",
       "  153: 'component',\n",
       "  154: 'traumatic',\n",
       "  155: 'woman',\n",
       "  156: 'health',\n",
       "  157: 'life',\n",
       "  158: 'woman',\n",
       "  159: 'holton',\n",
       "  160: 'fisher',\n",
       "  161: 'rowe',\n",
       "  162: 'would',\n",
       "  163: 'compromise',\n",
       "  164: 'bring',\n",
       "  165: 'pregnancy',\n",
       "  166: 'term',\n",
       "  167: 'indeed',\n",
       "  168: 'become',\n",
       "  169: 'mother',\n",
       "  170: 'many',\n",
       "  171: 'woman',\n",
       "  172: 'pregnancy',\n",
       "  173: 'result',\n",
       "  174: 'rape',\n",
       "  175: 'notably',\n",
       "  176: 'support',\n",
       "  177: 'highest',\n",
       "  178: 'gender',\n",
       "  179: 'role',\n",
       "  180: 'that',\n",
       "  181: 'complete',\n",
       "  182: 'women',\n",
       "  183: 'traumatic',\n",
       "  184: 'abortion',\n",
       "  185: 'higher',\n",
       "  186: 'than',\n",
       "  187: 'support',\n",
       "  188: 'abortion',\n",
       "  189: 'that',\n",
       "  190: 'chrisler',\n",
       "  191: 'gorman',\n",
       "  192: 'marván',\n",
       "  193: 'johnston',\n",
       "  194: 'robledo',\n",
       "  195: 'view',\n",
       "  196: 'elective',\n",
       "  197: 'bahr',\n",
       "  198: 'marcos',\n",
       "  199: 'craig',\n",
       "  200: 'accordingly',\n",
       "  201: 'women',\n",
       "  202: 'decide',\n",
       "  203: 'least',\n",
       "  204: 'temporarily',\n",
       "  205: 'reject',\n",
       "  206: 'this',\n",
       "  207: 'sacred',\n",
       "  208: 'role',\n",
       "  209: 'choose',\n",
       "  210: 'terminate',\n",
       "  211: 'pregnancy',\n",
       "  212: 'current',\n",
       "  213: 'study',\n",
       "  214: 'contribute',\n",
       "  215: 'this',\n",
       "  216: 'debate',\n",
       "  217: 'directly',\n",
       "  218: 'often',\n",
       "  219: 'meet',\n",
       "  220: 'with',\n",
       "  221: 'criticism',\n",
       "  222: 'osborne',\n",
       "  223: 'davies',\n",
       "  224: 'examine',\n",
       "  225: 'impact',\n",
       "  226: 'people',\n",
       "  227: 'gender',\n",
       "  228: 'role',\n",
       "  229: 'attitudes',\n",
       "  230: 'have',\n",
       "  231: 'observations',\n",
       "  232: 'such',\n",
       "  233: 'these',\n",
       "  234: 'lead',\n",
       "  235: 'cook',\n",
       "  236: 'jelen',\n",
       "  237: 'wilcox',\n",
       "  238: 'their',\n",
       "  239: 'support',\n",
       "  240: 'both',\n",
       "  241: 'elective',\n",
       "  242: 'traumatic',\n",
       "  243: 'abortion',\n",
       "  244: 'within',\n",
       "  245: 'contend',\n",
       "  246: 'that',\n",
       "  247: 'abortion',\n",
       "  248: 'debate',\n",
       "  249: 'general',\n",
       "  250: 'longitudinal',\n",
       "  251: 'panel',\n",
       "  252: 'study',\n",
       "  253: 'follow',\n",
       "  254: 'cross',\n",
       "  255: 'sectional',\n",
       "  256: 'public',\n",
       "  257: 'partially',\n",
       "  258: 'about',\n",
       "  259: 'appropriate',\n",
       "  260: 'roles',\n",
       "  261: 'women',\n",
       "  262: 'study',\n",
       "  263: 'before',\n",
       "  264: 'describe',\n",
       "  265: 'study',\n",
       "  266: 'examine',\n",
       "  267: 'stan',\n",
       "  268: 'society',\n",
       "  269: 'short',\n",
       "  270: 'traditional',\n",
       "  271: 'gender',\n",
       "  272: 'roles',\n",
       "  273: 'function',\n",
       "  274: 'dard',\n",
       "  275: 'socio',\n",
       "  276: 'demographic',\n",
       "  277: 'correlate',\n",
       "  278: 'abortion',\n",
       "  279: 'attitudes',\n",
       "  280: 'restrict',\n",
       "  281: 'women',\n",
       "  282: 'reproductive',\n",
       "  283: 'choices',\n",
       "  284: 'then',\n",
       "  285: 'introduce',\n",
       "  286: 'literature',\n",
       "  287: 'ambivalent',\n",
       "  288: 'sexism',\n",
       "  289: 'theory',\n",
       "  290: 'when',\n",
       "  291: 'discuss',\n",
       "  292: 'debate',\n",
       "  293: 'over',\n",
       "  294: 'woman',\n",
       "  295: 'right',\n",
       "  296: 'glick',\n",
       "  297: 'fiske',\n",
       "  298: 'reconcile',\n",
       "  299: 'some',\n",
       "  300: 'inconsisten',\n",
       "  301: 'choose',\n",
       "  302: 'easy',\n",
       "  303: 'rely',\n",
       "  304: 'common',\n",
       "  305: 'nomenclature',\n",
       "  306: 'cies',\n",
       "  307: 'identify',\n",
       "  308: 'past',\n",
       "  309: 'research',\n",
       "  310: 'then',\n",
       "  311: 'conclude',\n",
       "  312: 'this',\n",
       "  313: 'section',\n",
       "  314: 'life',\n",
       "  315: 'choice',\n",
       "  316: 'imply',\n",
       "  317: 'that',\n",
       "  318: 'abortion',\n",
       "  319: 'attitudes',\n",
       "  320: 'relatively',\n",
       "  321: 'uniform',\n",
       "  322: 'study',\n",
       "  323: 'demonstrate',\n",
       "  324: 'university',\n",
       "  325: 'auckland',\n",
       "  326: 'zealand',\n",
       "  327: 'ever',\n",
       "  328: 'that',\n",
       "  329: 'support',\n",
       "  330: 'abortion',\n",
       "  331: 'vary',\n",
       "  332: 'circumstances',\n",
       "  333: 'university',\n",
       "  334: 'british',\n",
       "  335: 'columbia',\n",
       "  336: 'okanagan',\n",
       "  337: 'kelowna',\n",
       "  338: 'canada',\n",
       "  339: 'surround',\n",
       "  340: 'woman',\n",
       "  341: 'decision',\n",
       "  342: 'terminate',\n",
       "  343: 'pregnancy',\n",
       "  344: 'correspond',\n",
       "  345: 'author',\n",
       "  346: 'alvarez',\n",
       "  347: 'brehm',\n",
       "  348: 'craig',\n",
       "  349: 'kane',\n",
       "  350: 'martinez',\n",
       "  351: 'yanshu',\n",
       "  352: 'huang',\n",
       "  353: 'school',\n",
       "  354: 'psychology',\n",
       "  355: 'university',\n",
       "  356: 'auckland',\n",
       "  357: 'private',\n",
       "  358: 'such',\n",
       "  359: 'circumstances',\n",
       "  360: 'include',\n",
       "  361: 'those',\n",
       "  362: 'that',\n",
       "  363: 'elective',\n",
       "  364: 'auckland',\n",
       "  365: 'zealand',\n",
       "  366: 'financial',\n",
       "  367: 'insecurity',\n",
       "  368: 'woman',\n",
       "  369: 'want',\n",
       "  370: 'child',\n",
       "  371: 'email',\n",
       "  372: 'yhua',\n",
       "  373: 'aucklanduni',\n",
       "  374: 'huang',\n",
       "  375: 'describe',\n",
       "  376: 'current',\n",
       "  377: 'program',\n",
       "  378: 'research',\n",
       "  379: 'associate',\n",
       "  380: 'ambivalent',\n",
       "  381: 'sexism',\n",
       "  382: 'theory',\n",
       "  383: 'hypotheses',\n",
       "  384: 'help',\n",
       "  385: 'establish',\n",
       "  386: 'direction',\n",
       "  387: 'maintenance',\n",
       "  388: 'traditional',\n",
       "  389: 'gender',\n",
       "  390: 'relationship',\n",
       "  391: 'between',\n",
       "  392: 'ambivalent',\n",
       "  393: 'sexism',\n",
       "  394: 'abortion',\n",
       "  395: 'roles',\n",
       "  396: 'attitudes',\n",
       "  397: 'study',\n",
       "  398: 'identify',\n",
       "  399: 'process',\n",
       "  400: 'through',\n",
       "  401: 'which',\n",
       "  402: 'these',\n",
       "  403: 'relationships',\n",
       "  404: 'occur',\n",
       "  405: 'study',\n",
       "  406: 'these',\n",
       "  407: 'although',\n",
       "  408: 'gender',\n",
       "  409: 'role',\n",
       "  410: 'attitudes',\n",
       "  411: 'critical',\n",
       "  412: 'predictors',\n",
       "  413: 'turn',\n",
       "  414: 'review',\n",
       "  415: 'literature',\n",
       "  416: 'correlate',\n",
       "  417: 'attitudes',\n",
       "  418: 'position',\n",
       "  419: 'abortion',\n",
       "  420: 'debate',\n",
       "  421: 'ambivalent',\n",
       "  422: 'sexism',\n",
       "  423: 'theory',\n",
       "  424: 'toward',\n",
       "  425: 'abortion',\n",
       "  426: 'glick',\n",
       "  427: 'fiske',\n",
       "  428: 'argue',\n",
       "  429: 'that',\n",
       "  430: 'women',\n",
       "  431: 'subordination',\n",
       "  432: 'maintain',\n",
       "  433: 'complementary',\n",
       "  434: 'ideologies',\n",
       "  435: 'hostile',\n",
       "  436: 'sexism',\n",
       "  437: 'benevolent',\n",
       "  438: 'sexism',\n",
       "  439: 'whereas',\n",
       "  440: 'correlate',\n",
       "  441: 'abortion',\n",
       "  442: 'attitudes',\n",
       "  443: 'punish',\n",
       "  444: 'norm',\n",
       "  445: 'violate',\n",
       "  446: 'women',\n",
       "  447: 'root',\n",
       "  448: 'idealize',\n",
       "  449: 'numerous',\n",
       "  450: 'demographic',\n",
       "  451: 'attitudinal',\n",
       "  452: 'variables',\n",
       "  453: 'have',\n",
       "  454: 'view',\n",
       "  455: 'women',\n",
       "  456: 'conform',\n",
       "  457: 'traditional',\n",
       "  458: 'gender',\n",
       "  459: 'roles',\n",
       "  460: 'associate',\n",
       "  461: 'with',\n",
       "  462: 'attitudes',\n",
       "  463: 'toward',\n",
       "  464: 'abortion',\n",
       "  465: 'perhaps',\n",
       "  466: 'unsurpris',\n",
       "  467: 'thus',\n",
       "  468: 'constrain',\n",
       "  469: 'women',\n",
       "  470: 'right',\n",
       "  471: 'praise',\n",
       "  472: 'those',\n",
       "  473: 'ingly',\n",
       "  474: 'research',\n",
       "  475: 'indicate',\n",
       "  476: 'that',\n",
       "  477: 'church',\n",
       "  478: 'attendance',\n",
       "  479: 'ellison',\n",
       "  480: 'adhere',\n",
       "  481: 'traditional',\n",
       "  482: 'gender',\n",
       "  483: 'roles',\n",
       "  484: 'while',\n",
       "  485: 'devalue',\n",
       "  486: 'those',\n",
       "  487: 'echevarria',\n",
       "  488: 'smith',\n",
       "  489: 'lynxwiler',\n",
       "  490: 'jelen',\n",
       "  491: 'violate',\n",
       "  492: 'these',\n",
       "  493: 'idealize',\n",
       "  494: 'prescriptions',\n",
       "  495: 'glick',\n",
       "  496: 'diebold',\n",
       "  497: 'wilcox',\n",
       "  498: 'religiosity',\n",
       "  499: 'general',\n",
       "  500: 'adebayo',\n",
       "  501: 'bailey',\n",
       "  502: 'werner',\n",
       "  503: 'nevertheless',\n",
       "  504: 'because',\n",
       "  505: 'benin',\n",
       "  506: 'esposito',\n",
       "  507: 'basow',\n",
       "  508: 'strickler',\n",
       "  509: 'danigelis',\n",
       "  510: 'cloak',\n",
       "  511: 'superficially',\n",
       "  512: 'positive',\n",
       "  513: 'tone',\n",
       "  514: 'often',\n",
       "  515: 'escape',\n",
       "  516: 'negatively',\n",
       "  517: 'associate',\n",
       "  518: 'with',\n",
       "  519: 'abortion',\n",
       "  520: 'support',\n",
       "  521: 'label',\n",
       "  522: 'sexism',\n",
       "  523: 'likewise',\n",
       "  524: 'political',\n",
       "  525: 'view',\n",
       "  526: 'consistently',\n",
       "  527: 'emerge',\n",
       "  528: 'predic',\n",
       "  529: 'covert',\n",
       "  530: 'form',\n",
       "  531: 'facilitate',\n",
       "  532: 'maintenance',\n",
       "  533: 'tors',\n",
       "  534: 'abortion',\n",
       "  535: 'attitudes',\n",
       "  536: 'specifically',\n",
       "  537: 'conservatism',\n",
       "  538: 'both',\n",
       "  539: 'inequalities',\n",
       "  540: 'oftentimes',\n",
       "  541: 'with',\n",
       "  542: 'little',\n",
       "  543: 'opposition',\n",
       "  544: 'from',\n",
       "  545: 'term',\n",
       "  546: 'ideology',\n",
       "  547: 'party',\n",
       "  548: 'identification',\n",
       "  549: 'negatively',\n",
       "  550: 'asso',\n",
       "  551: 'oppress',\n",
       "  552: 'also',\n",
       "  553: 'jackman',\n",
       "  554: 'study',\n",
       "  555: 'where',\n",
       "  556: 'partici',\n",
       "  557: 'ciated',\n",
       "  558: 'with',\n",
       "  559: 'abortion',\n",
       "  560: 'support',\n",
       "  561: 'hess',\n",
       "  562: 'rueb',\n",
       "  563: 'sahar',\n",
       "  564: 'pant',\n",
       "  565: 'evaluate',\n",
       "  566: 'passages',\n",
       "  567: 'contain',\n",
       "  568: 'either',\n",
       "  569: 'atti',\n",
       "  570: 'karasawa',\n",
       "  571: 'zucker',\n",
       "  572: 'indeed',\n",
       "  573: 'negative',\n",
       "  574: 'rela',\n",
       "  575: 'tudes',\n",
       "  576: 'barreto',\n",
       "  577: 'ellemers',\n",
       "  578: 'show',\n",
       "  579: 'that',\n",
       "  580: 'tionship',\n",
       "  581: 'between',\n",
       "  582: 'political',\n",
       "  583: 'conservatism',\n",
       "  584: 'support',\n",
       "  585: 'more',\n",
       "  586: 'prejudice',\n",
       "  587: 'than',\n",
       "  588: 'women',\n",
       "  589: 'also',\n",
       "  590: 'felt',\n",
       "  591: 'greater',\n",
       "  592: 'anger',\n",
       "  593: 'woman',\n",
       "  594: 'right',\n",
       "  595: 'choose',\n",
       "  596: 'find',\n",
       "  597: 'across',\n",
       "  598: 'culture',\n",
       "  599: 'agostino',\n",
       "  600: 'toward',\n",
       "  601: 'passage',\n",
       "  602: 'less',\n",
       "  603: 'anger',\n",
       "  604: 'toward',\n",
       "  605: 'wahlberg',\n",
       "  606: 'sage',\n",
       "  607: 'than',\n",
       "  608: 'another',\n",
       "  609: 'study',\n",
       "  610: 'find',\n",
       "  611: 'that',\n",
       "  612: 'women',\n",
       "  613: 'despite',\n",
       "  614: 'consistencies',\n",
       "  615: 'find',\n",
       "  616: 'literature',\n",
       "  617: 'term',\n",
       "  618: 'encounter',\n",
       "  619: 'negative',\n",
       "  620: 'hostile',\n",
       "  621: 'attitudes',\n",
       "  622: 'from',\n",
       "  623: 'increase',\n",
       "  624: 'religious',\n",
       "  625: 'political',\n",
       "  626: 'correlate',\n",
       "  627: 'abortion',\n",
       "  628: 'attitudes',\n",
       "  629: 'their',\n",
       "  630: 'endorsement',\n",
       "  631: 'fischer',\n",
       "  632: 'study',\n",
       "  633: 'examine',\n",
       "  634: 'gender',\n",
       "  635: 'differences',\n",
       "  636: 'abortion',\n",
       "  637: 'debate',\n",
       "  638: 'together',\n",
       "  639: 'these',\n",
       "  640: 'study',\n",
       "  641: 'demonstrate',\n",
       "  642: 'that',\n",
       "  643: 'many',\n",
       "  644: 'have',\n",
       "  645: 'yield',\n",
       "  646: 'inconsistent',\n",
       "  647: 'result',\n",
       "  648: 'whereas',\n",
       "  649: 'some',\n",
       "  650: 'find',\n",
       "  651: 'that',\n",
       "  652: 'sometimes',\n",
       "  653: 'system',\n",
       "  654: 'women',\n",
       "  655: 'more',\n",
       "  656: 'supportive',\n",
       "  657: 'than',\n",
       "  658: 'woman',\n",
       "  659: 'right',\n",
       "  660: 'because',\n",
       "  661: 'women',\n",
       "  662: 'often',\n",
       "  663: 'evaluate',\n",
       "  664: 'through',\n",
       "  665: 'polarize',\n",
       "  666: 'choose',\n",
       "  667: 'patel',\n",
       "  668: 'johns',\n",
       "  669: 'patel',\n",
       "  670: 'kooverjee',\n",
       "  671: 'view',\n",
       "  672: 'derogation',\n",
       "  673: 'admiration',\n",
       "  674: 'establish',\n",
       "  675: 'show',\n",
       "  676: 'that',\n",
       "  677: 'gender',\n",
       "  678: 'unassociated',\n",
       "  679: 'with',\n",
       "  680: 'abortion',\n",
       "  681: 'attitudes',\n",
       "  682: 'respectively',\n",
       "  683: 'glick',\n",
       "  684: 'women',\n",
       "  685: 'level',\n",
       "  686: 'benin',\n",
       "  687: 'ebaugh',\n",
       "  688: 'haney',\n",
       "  689: 'esposito',\n",
       "  690: 'basow',\n",
       "  691: 'ambivalent',\n",
       "  692: 'sexism',\n",
       "  693: 'often',\n",
       "  694: 'associate',\n",
       "  695: 'with',\n",
       "  696: 'their',\n",
       "  697: 'tendency',\n",
       "  698: 'finlay',\n",
       "  699: 'note',\n",
       "  700: 'however',\n",
       "  701: 'that',\n",
       "  702: 'though',\n",
       "  703: 'gender',\n",
       "  704: 'subtype',\n",
       "  705: 'women',\n",
       "  706: 'specifically',\n",
       "  707: 'becker',\n",
       "  708: 'demonstrate',\n",
       "  709: 'tend',\n",
       "  710: 'unassociated',\n",
       "  711: 'with',\n",
       "  712: 'support',\n",
       "  713: 'abortion',\n",
       "  714: 'most',\n",
       "  715: 'that',\n",
       "  716: 'women',\n",
       "  717: 'more',\n",
       "  718: 'likely',\n",
       "  719: 'endorse',\n",
       "  720: 'beliefs',\n",
       "  721: 'case',\n",
       "  722: 'women',\n",
       "  723: 'more',\n",
       "  724: 'supportive',\n",
       "  725: 'than',\n",
       "  726: 'abortion',\n",
       "  727: 'response',\n",
       "  728: 'subtypes',\n",
       "  729: 'women',\n",
       "  730: 'hold',\n",
       "  731: 'position',\n",
       "  732: 'that',\n",
       "  733: 'when',\n",
       "  734: 'termination',\n",
       "  735: 'pregnancy',\n",
       "  736: 'seek',\n",
       "  737: 'specific',\n",
       "  738: 'incongruent',\n",
       "  739: 'with',\n",
       "  740: 'traditional',\n",
       "  741: 'gender',\n",
       "  742: 'roles',\n",
       "  743: 'more',\n",
       "  744: 'reason',\n",
       "  745: 'elective',\n",
       "  746: 'abortion',\n",
       "  747: 'thus',\n",
       "  748: 'relationship',\n",
       "  749: 'between',\n",
       "  750: 'likely',\n",
       "  751: 'endorse',\n",
       "  752: 'reaction',\n",
       "  753: 'subtypes',\n",
       "  754: 'women',\n",
       "  755: 'gender',\n",
       "  756: 'abortion',\n",
       "  757: 'attitudes',\n",
       "  758: 'more',\n",
       "  759: 'complex',\n",
       "  760: 'than',\n",
       "  761: 'appear',\n",
       "  762: 'conform',\n",
       "  763: 'traditional',\n",
       "  764: 'roles',\n",
       "  765: 'these',\n",
       "  766: 'find',\n",
       "  767: 'corroborate',\n",
       "  768: 'first',\n",
       "  769: 'blush',\n",
       "  770: 'sibley',\n",
       "  771: 'wilson',\n",
       "  772: 'research',\n",
       "  773: 'show',\n",
       "  774: 'that',\n",
       "  775: 'these',\n",
       "  776: 'gender',\n",
       "  777: 'role',\n",
       "  778: 'attitudes',\n",
       "  779: 'predict',\n",
       "  780: 'expose',\n",
       "  781: 'women',\n",
       "  782: 'violate',\n",
       "  783: 'traditional',\n",
       "  784: 'gender',\n",
       "  785: 'roles',\n",
       "  786: 'position',\n",
       "  787: 'abortion',\n",
       "  788: 'debate',\n",
       "  789: 'better',\n",
       "  790: 'than',\n",
       "  791: 'gender',\n",
       "  792: 'itself',\n",
       "  793: 'express',\n",
       "  794: 'higher',\n",
       "  795: 'level',\n",
       "  796: 'lower',\n",
       "  797: 'level',\n",
       "  798: 'than',\n",
       "  799: 'accordingly',\n",
       "  800: 'wall',\n",
       "  801: 'colleagues',\n",
       "  802: 'find',\n",
       "  803: 'that',\n",
       "  804: 'endorse',\n",
       "  805: 'expose',\n",
       "  806: 'women',\n",
       "  807: 'conform',\n",
       "  808: 'traditional',\n",
       "  809: 'gender',\n",
       "  810: 'roles',\n",
       "  811: 'ment',\n",
       "  812: 'traditional',\n",
       "  813: 'gender',\n",
       "  814: 'roles',\n",
       "  815: 'associate',\n",
       "  816: 'with',\n",
       "  817: 'opposi',\n",
       "  818: 'other',\n",
       "  819: 'word',\n",
       "  820: 'ambivalent',\n",
       "  821: 'sexism',\n",
       "  822: 'encourage',\n",
       "  823: 'dichoto',\n",
       "  824: 'tion',\n",
       "  825: 'abortion',\n",
       "  826: 'likewise',\n",
       "  827: 'benin',\n",
       "  828: 'show',\n",
       "  829: 'that',\n",
       "  830: 'mization',\n",
       "  831: 'women',\n",
       "  832: 'into',\n",
       "  833: 'saint',\n",
       "  834: 'sinners',\n",
       "  835: 'traditional',\n",
       "  836: 'gender',\n",
       "  837: 'role',\n",
       "  838: 'attitudes',\n",
       "  839: 'negatively',\n",
       "  840: 'correlate',\n",
       "  841: 'with',\n",
       "  842: 'support',\n",
       "  843: 'elective',\n",
       "  844: 'abortion',\n",
       "  845: 'whereas',\n",
       "  846: 'krishnan',\n",
       "  847: 'sanctity',\n",
       "  848: 'motherhood',\n",
       "  849: 'find',\n",
       "  850: 'that',\n",
       "  851: 'traditional',\n",
       "  852: 'gender',\n",
       "  853: 'role',\n",
       "  854: 'attitudes',\n",
       "  855: 'predict',\n",
       "  856: 'opposi',\n",
       "  857: 'tion',\n",
       "  858: 'both',\n",
       "  859: 'elective',\n",
       "  860: 'traumatic',\n",
       "  861: 'abortion',\n",
       "  862: 'finally',\n",
       "  863: 'patel',\n",
       "  864: 'which',\n",
       "  865: 'women',\n",
       "  866: 'anoint',\n",
       "  867: 'saint',\n",
       "  868: 'like',\n",
       "  869: 'status',\n",
       "  870: 'johns',\n",
       "  871: 'show',\n",
       "  872: 'that',\n",
       "  873: 'women',\n",
       "  874: 'endorsement',\n",
       "  875: 'through',\n",
       "  876: 'their',\n",
       "  877: 'adherence',\n",
       "  878: 'traditional',\n",
       "  879: 'gender',\n",
       "  880: 'roles',\n",
       "  881: 'associate',\n",
       "  882: 'egalitarian',\n",
       "  883: 'gender',\n",
       "  884: 'roles',\n",
       "  885: 'positively',\n",
       "  886: 'associate',\n",
       "  887: 'with',\n",
       "  888: 'with',\n",
       "  889: 'motherhood',\n",
       "  890: 'indeed',\n",
       "  891: 'women',\n",
       "  892: 'reproductive',\n",
       "  893: 'port',\n",
       "  894: 'women',\n",
       "  895: 'autonomy',\n",
       "  896: 'abortion',\n",
       "  897: 'debate',\n",
       "  898: 'these',\n",
       "  899: 'describe',\n",
       "  900: 'with',\n",
       "  901: 'more',\n",
       "  902: 'warm',\n",
       "  903: 'positive',\n",
       "  904: 'attribute',\n",
       "  905: 'find',\n",
       "  906: 'highlight',\n",
       "  907: 'unique',\n",
       "  908: 'negative',\n",
       "  909: 'association',\n",
       "  910: 'between',\n",
       "  911: 'than',\n",
       "  912: 'their',\n",
       "  913: 'counterparts',\n",
       "  914: 'past',\n",
       "  915: 'reproduction',\n",
       "  916: 'traditional',\n",
       "  917: 'gender',\n",
       "  918: 'role',\n",
       "  919: 'attitudes',\n",
       "  920: 'support',\n",
       "  921: 'legalize',\n",
       "  922: 'chrisler',\n",
       "  923: 'chrisler',\n",
       "  924: 'also',\n",
       "  925: 'find',\n",
       "  926: 'that',\n",
       "  927: 'pregnant',\n",
       "  928: 'abortion',\n",
       "  929: 'women',\n",
       "  930: 'mother',\n",
       "  931: 'young',\n",
       "  932: 'infants',\n",
       "  933: 'more',\n",
       "  934: 'personality',\n",
       "  935: 'social',\n",
       "  936: 'psychology',\n",
       "  937: 'bulletin',\n",
       "  938: 'more',\n",
       "  939: 'likely',\n",
       "  940: 'complete',\n",
       "  941: 'study',\n",
       "  942: 'well',\n",
       "  943: 'identify',\n",
       "  944: 'plausible',\n",
       "  945: 'mechanism',\n",
       "  946: 'respect',\n",
       "  947: 'their',\n",
       "  948: 'counterparts',\n",
       "  949: 'have',\n",
       "  950: 'hysterec',\n",
       "  951: 'behind',\n",
       "  952: 'their',\n",
       "  953: 'association',\n",
       "  954: 'study',\n",
       "  955: 'fill',\n",
       "  956: 'tomies',\n",
       "  957: 'post',\n",
       "  958: 'menopausal',\n",
       "  959: 'critically',\n",
       "  960: 'posi',\n",
       "  961: 'important',\n",
       "  962: 'literature',\n",
       "  963: 'specifically',\n",
       "  964: 'past',\n",
       "  965: 'research',\n",
       "  966: 'tively',\n",
       "  967: 'associate',\n",
       "  968: 'with',\n",
       "  969: 'favorable',\n",
       "  970: 'evaluations',\n",
       "  971: 'both',\n",
       "  972: 'women',\n",
       "  973: 'have',\n",
       "  974: 'solely',\n",
       "  975: 'assess',\n",
       "  976: 'association',\n",
       "  977: 'between',\n",
       "  978: 'ambivalent',\n",
       "  979: 'with',\n",
       "  980: 'young',\n",
       "  981: 'infants',\n",
       "  982: 'pregnant',\n",
       "  983: 'women',\n",
       "  984: 'abortion',\n",
       "  985: 'attitudes',\n",
       "  986: 'cross',\n",
       "  987: 'sectional',\n",
       "  988: 'data',\n",
       "  989: 'another',\n",
       "  990: 'which',\n",
       "  991: 'women',\n",
       "  992: 'pressure',\n",
       "  993: 'conform',\n",
       "  994: 'huang',\n",
       "  995: 'osborne',\n",
       "  996: 'davies',\n",
       "  997: 'such',\n",
       "  998: 'traditional',\n",
       "  999: 'gender',\n",
       "  ...}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = 'dict_data/word2vec' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old user data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = pd.read_csv('https://inroads-test-bucket1.s3.amazonaws.com/Taxonomy-Grid+view+(1).csv', dtype={'text': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                RecordID       Term      Domain      Subdomain\n",
       "0     rec8EgTumvHTEJdLa   abstract  Conceptual     Conceptual\n",
       "1     recMJFlgfo00roAuw    absence  Conceptual     Conceptual\n",
       "2     recAuJIeX88OP6ZIb  abundance  Conceptual     Conceptual\n",
       "3     recz0CPCI09cP5iUX    reality  Conceptual     Conceptual\n",
       "4     rec8Bm0aJwpSKdOGQ  beautiful  Conceptual     Conceptual\n",
       "...                 ...        ...         ...            ...\n",
       "1016  recMKWuPq8dMo70HD     summer    Temporal  Units of time\n",
       "1017  recjrhx9mX0O1xCu0     autumn    Temporal  Units of time\n",
       "1018  recraEJIJ7Fe5acU5       1971    Temporal  Units of time\n",
       "1019  recvyH9ENCss8n1xG       1972    Temporal  Units of time\n",
       "1020  recKH4SX6UXD9wvKv        NaN         NaN            NaN\n",
       "\n",
       "[1021 rows x 4 columns]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = old_data[['Term','Domain']].copy().sort_values(['Term','Domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.to_csv('inroads_old_data.csv')\n",
    "old_data.to_pickle('inroads_old.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "\n",
    "for i in range(0,len(old_data)):\n",
    "    temp = str(old_data.Term.iloc[i])     \n",
    "    temp = text_lowercase(temp) #make all lower\n",
    "    temp_2 = remove_numbers(temp) # remove all numbers\n",
    "    temp_3 = str.strip(temp_2) # remove white spaces 1\n",
    "    temp_4 = temp_3.replace('\\n',' ').replace(\"\\t\", \" \") # remove white spaces 2\n",
    "    temp_5 = re.sub(\" +\", \" \", temp_4) # remove white spaces 3\n",
    "    temp_6 = remove_punctuation(temp_5) # remove punctuation \n",
    "    temp_7 = lemmatize_word(temp_6) # lemmatize text\n",
    "    temp_8 = [word for word in temp_7 if word.isalnum()] # remove auxiliarly punctuation\n",
    "    temp_9 = remove_short_words(temp_8) # remove short words\n",
    "    result.append(list(filter(None, temp_9))) # remove empty strings and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data['clean_text'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724              []\n",
       "1018             []\n",
       "1019             []\n",
       "928     [september]\n",
       "682              []\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data.clean_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokens = []\n",
    "\n",
    "for i in range(0,len(old_data)):\n",
    "    text = old_data.clean_text[i]\n",
    "    #print(text)\n",
    "    old_tokens.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          tokens\n",
       "0      abstract\n",
       "1       absence\n",
       "2     abundance\n",
       "3       reality\n",
       "4     beautiful\n",
       "...         ...\n",
       "1307     winter\n",
       "1308     spring\n",
       "1309     season\n",
       "1310     summer\n",
       "1311     autumn\n",
       "\n",
       "[1312 rows x 1 columns]>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokens = pd.DataFrame(old_tokens,columns=['tokens'])\n",
    "old_tokens.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokens.to_pickle('old_tokens.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: boto in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.14.51)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.51 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.51)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.51->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.51->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.cluster import KMeans;\n",
    "#from sklearn.neighbors import KDTree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_list = list(final_inroads_clean.clean_text)\n",
    "token_list = pd.read_pickle(\"final_inroads_clean.pkl\").clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use the same model I use, which is saved, I provide code to download it from s3.  Or, you can create your own by uncommenting the training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_features = 300\n",
    "#min_word_count = 50\n",
    "#num_workers = 2\n",
    "#window_size = 6\n",
    "#subsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Vec(\n",
    "#    token_list, \n",
    "#    workers = num_workers, \n",
    "#    size = num_features, \n",
    "#    min_count=min_word_count, \n",
    "#    window=window_size, \n",
    "#    sample=subsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need both pieces to make the model work, I'm struggling to find a way for me to upload and download the gensim model from S3.  TO BE DETERMINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'s3.ServiceResource' object has no attribute 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-376c03e75914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_gensim_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-376c03e75914>\u001b[0m in \u001b[0;36mdownload_gensim_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                     aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'))\n\u001b[1;32m     12\u001b[0m     \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS3_BUCKET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS3_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 's3.ServiceResource' object has no attribute 'key'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import boto\n",
    "from boto.s3.connection import OrdinaryCallingFormat\n",
    "\n",
    "\n",
    "S3_BUCKET = 'inroads-test-bucket1'\n",
    "S3_KEY = 'w2v_model.bin'\n",
    "\n",
    "def download_gensim_model():\n",
    "    s3_conn = boto3.resource('s3', aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                    aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "    bucket = s3_conn.Bucket(S3_BUCKET)\n",
    "    key = s3_conn.get_key(S3_KEY)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(key, binary=True)\n",
    "    return model\n",
    "\n",
    "model = download_gensim_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy-Grid view (1).csv 2020-08-14 16:05:03+00:00\n",
      "clean_full_text.txt 2020-08-14 00:54:00+00:00\n",
      "clean_text.txt 2020-07-26 19:23:38+00:00\n",
      "clean_tokens.txt 2020-07-26 19:24:58+00:00\n",
      "cleaned_data.pkl 2020-07-16 20:34:30+00:00\n",
      "clusters/ 2020-07-27 22:01:22+00:00\n",
      "clusters/top_words_10.csv 2020-07-27 23:23:35+00:00\n",
      "clusters/top_words_11.csv 2020-07-27 23:23:35+00:00\n",
      "clusters/top_words_12.csv 2020-07-27 23:23:35+00:00\n",
      "clusters/top_words_13.csv 2020-08-19 18:51:52+00:00\n",
      "clusters/top_words_4.csv 2020-07-27 23:23:35+00:00\n",
      "clusters/top_words_5.csv 2020-07-27 23:23:35+00:00\n",
      "clusters/top_words_6.csv 2020-07-27 23:23:35+00:00\n",
      "clusters/top_words_7.csv 2020-07-29 16:29:33+00:00\n",
      "clusters/top_words_8.csv 2020-07-29 16:29:33+00:00\n",
      "clusters/top_words_9.csv 2020-08-19 18:51:52+00:00\n",
      "data/ 2020-08-19 18:52:05+00:00\n",
      "data/Vectorized/ 2020-08-19 19:01:15+00:00\n",
      "data/Vectorized/combined_vectorized 2020-08-23 21:25:07+00:00\n",
      "data/Vectorized/combined_vectorized.csv 2020-09-03 18:40:54+00:00\n",
      "data/Vectorized/inroads_vectorized.csv 2020-09-05 01:29:39+00:00\n",
      "data/Vectorized/old_vectorized.csv 2020-08-19 20:18:49+00:00\n",
      "data/Vectorized/old_vectorized.pkl 2020-08-19 19:12:15+00:00\n",
      "data/labeled_data.csv 2020-08-25 03:40:58+00:00\n",
      "data/raw/ 2020-08-19 18:52:09+00:00\n",
      "data/raw/combined_vectorized 2020-08-24 18:03:25+00:00\n",
      "data/raw/combined_vectorized.csv 2020-08-24 23:21:59+00:00\n",
      "data/raw/inroads_clean.pkl 2020-08-19 18:54:51+00:00\n",
      "data/raw/inroads_old.pkl 2020-08-19 18:55:05+00:00\n",
      "data/raw/inroads_tokens.pkl 2020-08-19 18:54:22+00:00\n",
      "data/raw/old_tokens.pkl 2020-08-19 18:55:14+00:00\n",
      "inroads_blog.csv 2020-06-17 18:36:33+00:00\n",
      "inroads_only.csv 2020-06-17 18:41:20+00:00\n",
      "inroads_word2vec_model 2020-08-17 14:23:03+00:00\n",
      "inroads_word2vec_model.wv.vectors.npy 2020-08-17 14:23:03+00:00\n",
      "kmeans/inroads-k-means-job-20200824001355/output/model.tar.gz 2020-08-24 00:17:41+00:00\n",
      "kmeans/inroads-k-means-job-20200824180400/output/model.tar.gz 2020-08-24 18:07:57+00:00\n",
      "kmeans/kmeans-2020-08-20-23-21-41-616/output/model.tar.gz 2020-08-20 23:25:32+00:00\n",
      "labeled_data 2020-08-25 02:00:33+00:00\n",
      "myDictionary 2020-08-18 23:44:18+00:00\n",
      "old_vectors.pkl 2020-08-18 21:59:17+00:00\n",
      "raw_data.csv 2020-06-04 21:53:21+00:00\n",
      "sagemaker/classification-byo/finalized_svm_model.sav/clf.tar.gz 2020-08-06 18:20:54+00:00\n",
      "shout_your_abort.csv 2020-06-17 18:32:52+00:00\n",
      "top_words.csv 2020-07-27 14:26:21+00:00\n",
      "top_words.txt 2020-07-27 14:16:15+00:00\n",
      "w2v_model.bin 2020-08-17 15:24:25+00:00\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('inroads-test-bucket1')\n",
    "for obj in bucket.objects.all():\n",
    "    print(obj.key, obj.last_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "head_object() argument after ** must be a mapping, not _io.BufferedRandom",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-436-4cfa9d6b3449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w+b'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inroads-test-bucket1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w2v_model.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_2.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mbucket_download_file\u001b[0;34m(self, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    244\u001b[0m     return self.meta.client.download_file(\n\u001b[1;32m    245\u001b[0m         \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    170\u001b[0m         return transfer.download_file(\n\u001b[1;32m    171\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    305\u001b[0m             bucket, key, filename, extra_args, subscribers)\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# exceeded we need to throw the same error from boto3 instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/download.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m             transfer_future.meta.provide_transfer_size(\n",
      "\u001b[0;31mTypeError\u001b[0m: head_object() argument after ** must be a mapping, not _io.BufferedRandom"
     ]
    }
   ],
   "source": [
    "#s3.Bucket('inroads-test-bucket1').download_file('w2v_model.bin', 'model_2.bin')\n",
    "#s3.Bucket('inroads-test-bucket1').download_file('inroads_word2vec_model', 'model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"inroads_word2vec_model\"\n",
    "model = Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abortions', 0.5601829290390015),\n",
       " ('postabortion', 0.4012739658355713),\n",
       " ('contraception', 0.3701522946357727),\n",
       " ('abor', 0.36626535654067993),\n",
       " ('medically', 0.34925901889801025),\n",
       " ('antenatal', 0.3442828059196472),\n",
       " ('termination', 0.3441293239593506),\n",
       " ('reproductive', 0.3407558798789978),\n",
       " ('misoprostol', 0.338686466217041),\n",
       " ('contraceptive', 0.33505553007125854)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"abortion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text):\n",
    "    vector_list = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        word = text[i]     \n",
    "        try: \n",
    "            vector = model.wv[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "            if vector is None:\n",
    "                vector = np.zeros(300,dtype=int) \n",
    "                vector_list.append([word, vector])\n",
    "            else: \n",
    "                vector_list.append([word, vector])\n",
    "        #vector_embed.append(pd.Series([word, vector], index=['token',range(0,300)]), ignore_index=True)\n",
    "                    \n",
    "    return vector_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokens = pd.read_pickle(\"old_tokens.pkl\")\n",
    "token_list_old = list(old_tokens.tokens)\n",
    "token_list_old = list(dict.fromkeys(token_list_old))\n",
    "len(token_list_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          token                                             vector         0  \\\n",
       "0     abstract  [0.009340104, -0.011540192, -0.005010838, 0.17...  0.009340   \n",
       "1      absence  [0.07312082, -0.047603212, 0.091147356, 0.1621...  0.073121   \n",
       "2    abundance  [0.013253822, 0.05022224, -0.08944513, 0.08348...  0.013254   \n",
       "3      reality  [0.017231788, -0.13826157, -0.06955002, 0.1615...  0.017232   \n",
       "4    beautiful  [-0.056970224, 0.02030195, -0.024677064, -0.04... -0.056970   \n",
       "..         ...                                                ...       ...   \n",
       "936     winter  [-0.021195991, -0.011310648, -0.07302275, 0.03... -0.021196   \n",
       "937     spring  [-0.028904684, 0.093232386, -0.058348857, -0.0... -0.028905   \n",
       "938     season  [-0.074280344, 0.093354344, -0.14504637, 0.006... -0.074280   \n",
       "939     summer  [-0.04597636, -0.006390778, -0.056196004, -0.0... -0.045976   \n",
       "940     autumn  [0.014605279, 0.1677676, -0.08277614, -0.01768...  0.014605   \n",
       "\n",
       "            1         2         3         4         5         6         7  \\\n",
       "0   -0.011540 -0.005011  0.170543 -0.070042 -0.022673  0.035479 -0.033519   \n",
       "1   -0.047603  0.091147  0.162190 -0.052149 -0.108441 -0.028311 -0.041613   \n",
       "2    0.050222 -0.089445  0.083483 -0.044747 -0.065899  0.019941 -0.052136   \n",
       "3   -0.138262 -0.069550  0.161501 -0.024385 -0.121781 -0.035268  0.007963   \n",
       "4    0.020302 -0.024677 -0.048865  0.064803 -0.033035 -0.029016 -0.020143   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "936 -0.011311 -0.073023  0.030679  0.117935  0.040947  0.024909  0.073402   \n",
       "937  0.093232 -0.058349 -0.013379 -0.086675  0.104365  0.064961  0.024115   \n",
       "938  0.093354 -0.145046  0.006174  0.000712  0.073791  0.088202  0.058804   \n",
       "939 -0.006391 -0.056196 -0.057425 -0.018857 -0.025275  0.079501  0.044921   \n",
       "940  0.167768 -0.082776 -0.017683 -0.045675  0.065280  0.051255 -0.045690   \n",
       "\n",
       "     ...       290       291       292       293       294       295  \\\n",
       "0    ...  0.027858  0.024611 -0.013685  0.042488  0.054621 -0.036383   \n",
       "1    ...  0.018856  0.007211 -0.080637  0.157568 -0.035902 -0.001056   \n",
       "2    ...  0.043838  0.002926 -0.050873 -0.012946 -0.007152  0.148603   \n",
       "3    ...  0.027821  0.020316 -0.028523  0.028286 -0.049850 -0.049433   \n",
       "4    ... -0.071911 -0.002086  0.023876  0.077053  0.024279 -0.043584   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "936  ... -0.079502  0.096267  0.060664  0.034208 -0.119383  0.066091   \n",
       "937  ...  0.079233 -0.081211 -0.045705  0.038298 -0.008489 -0.004637   \n",
       "938  ... -0.028003 -0.029557 -0.022236 -0.088852 -0.003609  0.031037   \n",
       "939  ...  0.089390  0.021395  0.031801  0.014382 -0.027417  0.028251   \n",
       "940  ... -0.044892 -0.049401  0.061406  0.076096  0.013065  0.041824   \n",
       "\n",
       "          296       297       298       299  \n",
       "0    0.018394 -0.045638  0.025203 -0.052491  \n",
       "1   -0.014947 -0.114950 -0.121481  0.031619  \n",
       "2    0.109223 -0.019303  0.029999  0.042774  \n",
       "3    0.133848  0.008867 -0.076711  0.042498  \n",
       "4   -0.053606  0.007344  0.018627 -0.046889  \n",
       "..        ...       ...       ...       ...  \n",
       "936 -0.133915  0.001732 -0.072120  0.064670  \n",
       "937 -0.072350  0.039764 -0.014958  0.071050  \n",
       "938  0.097972  0.119025 -0.085960 -0.042356  \n",
       "939 -0.089081 -0.034247 -0.061705 -0.033233  \n",
       "940 -0.068288 -0.040960 -0.040674  0.035157  \n",
       "\n",
       "[941 rows x 302 columns]>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_vectors = get_vector(token_list_old)\n",
    "\n",
    "old_vocab = list()\n",
    "\n",
    "for i in range(0,len(old_vectors)):\n",
    "    test = old_vectors[i]\n",
    "    old_vocab.append(test[0])\n",
    "\n",
    "old_vocab_df = pd.DataFrame(old_vocab, columns=[\"token\"])\n",
    "\n",
    "old_vectors_2 = list()\n",
    "\n",
    "for i in range(0,len(old_vectors)):\n",
    "    test = old_vectors[i]\n",
    "    old_vectors_2.append(test[1])\n",
    "    \n",
    "old_array = np.array(old_vectors_2)\n",
    "old_vectors_df = pd.DataFrame(old_vectors, columns=[\"token\", \"vector\"])\n",
    "\n",
    "old_vectors_df_2 = pd.DataFrame(old_vectors_df.vector.tolist())\n",
    "old_vectors_df_2.head\n",
    "\n",
    "result = pd.concat([old_vectors_df, old_vectors_df_2], axis=1)\n",
    "result.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009340</td>\n",
       "      <td>-0.011540</td>\n",
       "      <td>-0.005011</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>-0.070042</td>\n",
       "      <td>-0.022673</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>-0.033519</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.045745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027858</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>-0.013685</td>\n",
       "      <td>0.042488</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>-0.036383</td>\n",
       "      <td>0.018394</td>\n",
       "      <td>-0.045638</td>\n",
       "      <td>0.025203</td>\n",
       "      <td>-0.052491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.073121</td>\n",
       "      <td>-0.047603</td>\n",
       "      <td>0.091147</td>\n",
       "      <td>0.162190</td>\n",
       "      <td>-0.052149</td>\n",
       "      <td>-0.108441</td>\n",
       "      <td>-0.028311</td>\n",
       "      <td>-0.041613</td>\n",
       "      <td>-0.026646</td>\n",
       "      <td>-0.015232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>-0.080637</td>\n",
       "      <td>0.157568</td>\n",
       "      <td>-0.035902</td>\n",
       "      <td>-0.001056</td>\n",
       "      <td>-0.014947</td>\n",
       "      <td>-0.114950</td>\n",
       "      <td>-0.121481</td>\n",
       "      <td>0.031619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.050222</td>\n",
       "      <td>-0.089445</td>\n",
       "      <td>0.083483</td>\n",
       "      <td>-0.044747</td>\n",
       "      <td>-0.065899</td>\n",
       "      <td>0.019941</td>\n",
       "      <td>-0.052136</td>\n",
       "      <td>0.037055</td>\n",
       "      <td>-0.078439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043838</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>-0.050873</td>\n",
       "      <td>-0.012946</td>\n",
       "      <td>-0.007152</td>\n",
       "      <td>0.148603</td>\n",
       "      <td>0.109223</td>\n",
       "      <td>-0.019303</td>\n",
       "      <td>0.029999</td>\n",
       "      <td>0.042774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017232</td>\n",
       "      <td>-0.138262</td>\n",
       "      <td>-0.069550</td>\n",
       "      <td>0.161501</td>\n",
       "      <td>-0.024385</td>\n",
       "      <td>-0.121781</td>\n",
       "      <td>-0.035268</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>0.047652</td>\n",
       "      <td>-0.019447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027821</td>\n",
       "      <td>0.020316</td>\n",
       "      <td>-0.028523</td>\n",
       "      <td>0.028286</td>\n",
       "      <td>-0.049850</td>\n",
       "      <td>-0.049433</td>\n",
       "      <td>0.133848</td>\n",
       "      <td>0.008867</td>\n",
       "      <td>-0.076711</td>\n",
       "      <td>0.042498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.056970</td>\n",
       "      <td>0.020302</td>\n",
       "      <td>-0.024677</td>\n",
       "      <td>-0.048865</td>\n",
       "      <td>0.064803</td>\n",
       "      <td>-0.033035</td>\n",
       "      <td>-0.029016</td>\n",
       "      <td>-0.020143</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071911</td>\n",
       "      <td>-0.002086</td>\n",
       "      <td>0.023876</td>\n",
       "      <td>0.077053</td>\n",
       "      <td>0.024279</td>\n",
       "      <td>-0.043584</td>\n",
       "      <td>-0.053606</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>-0.046889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.013136</td>\n",
       "      <td>-0.014611</td>\n",
       "      <td>-0.012390</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>-0.045401</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>-0.069420</td>\n",
       "      <td>-0.037662</td>\n",
       "      <td>0.050937</td>\n",
       "      <td>-0.020486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>0.051054</td>\n",
       "      <td>0.041030</td>\n",
       "      <td>-0.028855</td>\n",
       "      <td>-0.004319</td>\n",
       "      <td>0.048255</td>\n",
       "      <td>-0.001835</td>\n",
       "      <td>0.036622</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>-0.002147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.086506</td>\n",
       "      <td>-0.052345</td>\n",
       "      <td>-0.011814</td>\n",
       "      <td>-0.034647</td>\n",
       "      <td>-0.079073</td>\n",
       "      <td>-0.058749</td>\n",
       "      <td>-0.012539</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>-0.086351</td>\n",
       "      <td>-0.025194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012264</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>-0.025577</td>\n",
       "      <td>0.011025</td>\n",
       "      <td>0.039866</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0.045241</td>\n",
       "      <td>-0.003326</td>\n",
       "      <td>0.046087</td>\n",
       "      <td>-0.122829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.123545</td>\n",
       "      <td>-0.013735</td>\n",
       "      <td>-0.113005</td>\n",
       "      <td>-0.184742</td>\n",
       "      <td>-0.081858</td>\n",
       "      <td>-0.060789</td>\n",
       "      <td>0.017910</td>\n",
       "      <td>0.059038</td>\n",
       "      <td>-0.041535</td>\n",
       "      <td>-0.106276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107440</td>\n",
       "      <td>0.065636</td>\n",
       "      <td>0.106605</td>\n",
       "      <td>-0.057781</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>0.081753</td>\n",
       "      <td>0.071285</td>\n",
       "      <td>-0.078630</td>\n",
       "      <td>0.021454</td>\n",
       "      <td>-0.010050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.114382</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>-0.012011</td>\n",
       "      <td>-0.072564</td>\n",
       "      <td>0.026916</td>\n",
       "      <td>-0.040016</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>-0.069554</td>\n",
       "      <td>0.014562</td>\n",
       "      <td>-0.100948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014017</td>\n",
       "      <td>-0.057527</td>\n",
       "      <td>0.088652</td>\n",
       "      <td>-0.042609</td>\n",
       "      <td>-0.056985</td>\n",
       "      <td>-0.040120</td>\n",
       "      <td>-0.005427</td>\n",
       "      <td>-0.015367</td>\n",
       "      <td>-0.112244</td>\n",
       "      <td>0.004891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.042463</td>\n",
       "      <td>-0.083875</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>-0.123742</td>\n",
       "      <td>-0.062383</td>\n",
       "      <td>-0.070657</td>\n",
       "      <td>0.028724</td>\n",
       "      <td>-0.030846</td>\n",
       "      <td>0.041368</td>\n",
       "      <td>-0.086228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068538</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>-0.084415</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>-0.022164</td>\n",
       "      <td>0.012042</td>\n",
       "      <td>-0.073328</td>\n",
       "      <td>-0.062452</td>\n",
       "      <td>0.048240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.009340 -0.011540 -0.005011  0.170543 -0.070042 -0.022673  0.035479   \n",
       "1   0.073121 -0.047603  0.091147  0.162190 -0.052149 -0.108441 -0.028311   \n",
       "2   0.013254  0.050222 -0.089445  0.083483 -0.044747 -0.065899  0.019941   \n",
       "3   0.017232 -0.138262 -0.069550  0.161501 -0.024385 -0.121781 -0.035268   \n",
       "4  -0.056970  0.020302 -0.024677 -0.048865  0.064803 -0.033035 -0.029016   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.013136 -0.014611 -0.012390  0.006732 -0.045401  0.076660 -0.069420   \n",
       "96 -0.086506 -0.052345 -0.011814 -0.034647 -0.079073 -0.058749 -0.012539   \n",
       "97 -0.123545 -0.013735 -0.113005 -0.184742 -0.081858 -0.060789  0.017910   \n",
       "98 -0.114382  0.004027 -0.012011 -0.072564  0.026916 -0.040016  0.049948   \n",
       "99 -0.042463 -0.083875  0.024914 -0.123742 -0.062383 -0.070657  0.028724   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "0  -0.033519  0.038251  0.045745  ...  0.027858  0.024611 -0.013685  0.042488   \n",
       "1  -0.041613 -0.026646 -0.015232  ...  0.018856  0.007211 -0.080637  0.157568   \n",
       "2  -0.052136  0.037055 -0.078439  ...  0.043838  0.002926 -0.050873 -0.012946   \n",
       "3   0.007963  0.047652 -0.019447  ...  0.027821  0.020316 -0.028523  0.028286   \n",
       "4  -0.020143  0.038997  0.015680  ... -0.071911 -0.002086  0.023876  0.077053   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95 -0.037662  0.050937 -0.020486  ...  0.029552  0.051054  0.041030 -0.028855   \n",
       "96  0.001059 -0.086351 -0.025194  ...  0.012264  0.015519 -0.025577  0.011025   \n",
       "97  0.059038 -0.041535 -0.106276  ...  0.107440  0.065636  0.106605 -0.057781   \n",
       "98 -0.069554  0.014562 -0.100948  ... -0.014017 -0.057527  0.088652 -0.042609   \n",
       "99 -0.030846  0.041368 -0.086228  ...  0.068538  0.004217  0.075371 -0.084415   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "0   0.054621 -0.036383  0.018394 -0.045638  0.025203 -0.052491  \n",
       "1  -0.035902 -0.001056 -0.014947 -0.114950 -0.121481  0.031619  \n",
       "2  -0.007152  0.148603  0.109223 -0.019303  0.029999  0.042774  \n",
       "3  -0.049850 -0.049433  0.133848  0.008867 -0.076711  0.042498  \n",
       "4   0.024279 -0.043584 -0.053606  0.007344  0.018627 -0.046889  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95 -0.004319  0.048255 -0.001835  0.036622  0.038000 -0.002147  \n",
       "96  0.039866  0.007830  0.045241 -0.003326  0.046087 -0.122829  \n",
       "97  0.029954  0.081753  0.071285 -0.078630  0.021454 -0.010050  \n",
       "98 -0.056985 -0.040120 -0.005427 -0.015367 -0.112244  0.004891  \n",
       "99  0.004755 -0.022164  0.012042 -0.073328 -0.062452  0.048240  \n",
       "\n",
       "[100 rows x 300 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_vectors_df_2[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6BF0D1CAA6D854BD',\n",
       "  'HostId': 'OD5qEMMfePzcMBQ6gbr1759QzWhiYnYZFK1igiN+NcJfuNY751cVeCTXARJkXnacSk2vg2+WqSo=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'OD5qEMMfePzcMBQ6gbr1759QzWhiYnYZFK1igiN+NcJfuNY751cVeCTXARJkXnacSk2vg2+WqSo=',\n",
       "   'x-amz-request-id': '6BF0D1CAA6D854BD',\n",
       "   'date': 'Sat, 05 Sep 2020 01:33:43 GMT',\n",
       "   'etag': '\"4126fa3c0f3f0829b135695480016e51\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"4126fa3c0f3f0829b135695480016e51\"'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'inroads-test-bucket1' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "result.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'data/Vectorized/old_vectorized.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13196"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_inroads_tokens = pd.read_pickle(\"clean_inroads_tokens.pkl\")\n",
    "token_list_inroads = list(clean_inroads_tokens.tokens)\n",
    "token_list_inroads = list(dict.fromkeys(token_list_inroads))\n",
    "len(token_list_inroads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "inroads_vectors = get_vector(token_list_inroads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of              tokens                                             vector  \\\n",
       "0          research  [0.18854871, -0.0510548, -0.051034868, 0.01212...   \n",
       "1           article  [-0.08915062, -0.032444216, 0.0660668, -0.0830...   \n",
       "2       personality  [0.080988534, 0.07145718, -0.036697134, 0.0516...   \n",
       "3            social  [0.04818078, -0.024814632, -0.042576127, 0.044...   \n",
       "4        psychology  [-0.0020839642, 0.08448851, 0.014705653, 0.073...   \n",
       "...             ...                                                ...   \n",
       "10056         saudi  [-0.014218387, -0.024083603, 0.08134469, -0.06...   \n",
       "10057        arabia  [-0.07335844, -0.063884735, 0.05745144, -0.059...   \n",
       "10058      adultery  [-0.08255755, 0.04301242, -0.11898199, -0.1686...   \n",
       "10059      indecent  [0.010403189, -0.0015966488, 0.022658184, 0.14...   \n",
       "10060  islamisation  [0.013339531, 0.02428078, -0.089154445, 0.0554...   \n",
       "\n",
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.188549 -0.051055 -0.051035  0.012129  0.018650 -0.108756 -0.054335   \n",
       "1     -0.089151 -0.032444  0.066067 -0.083048 -0.025339 -0.115493 -0.082126   \n",
       "2      0.080989  0.071457 -0.036697  0.051603 -0.098144 -0.004233  0.047631   \n",
       "3      0.048181 -0.024815 -0.042576  0.044887 -0.072843 -0.014483 -0.092501   \n",
       "4     -0.002084  0.084489  0.014706  0.073587 -0.098434 -0.009841 -0.012736   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10056 -0.014218 -0.024084  0.081345 -0.069396 -0.051680  0.006385 -0.056575   \n",
       "10057 -0.073358 -0.063885  0.057451 -0.059011 -0.102870  0.027484 -0.090268   \n",
       "10058 -0.082558  0.043012 -0.118982 -0.168650  0.035012  0.004704  0.010404   \n",
       "10059  0.010403 -0.001597  0.022658  0.142871  0.002488 -0.039659 -0.024233   \n",
       "10060  0.013340  0.024281 -0.089154  0.055416  0.031318  0.012631  0.109731   \n",
       "\n",
       "              7  ...       290       291       292       293       294  \\\n",
       "0     -0.003939  ...  0.067170  0.014436  0.027066 -0.011628 -0.076336   \n",
       "1     -0.015637  ... -0.031248  0.035671  0.022658  0.047292 -0.071116   \n",
       "2      0.025854  ... -0.020227  0.035703 -0.040952 -0.026335  0.001954   \n",
       "3     -0.036309  ...  0.001294 -0.023781 -0.000153 -0.002167  0.062039   \n",
       "4     -0.084855  ...  0.072306 -0.008926 -0.072427 -0.022924 -0.045194   \n",
       "...         ...  ...       ...       ...       ...       ...       ...   \n",
       "10056  0.193053  ... -0.052560 -0.098588  0.010969  0.000303  0.012525   \n",
       "10057  0.049498  ... -0.018132 -0.060725 -0.017951 -0.002820  0.020353   \n",
       "10058  0.037006  ... -0.002580 -0.025475 -0.014083 -0.033139  0.047937   \n",
       "10059  0.012931  ...  0.044288 -0.061349  0.003900 -0.058371 -0.064461   \n",
       "10060  0.014341  ...  0.068354 -0.119120  0.064430 -0.006576 -0.002567   \n",
       "\n",
       "            295       296       297       298       299  \n",
       "0      0.020410  0.032563 -0.045787 -0.077532 -0.084312  \n",
       "1     -0.075786  0.015880  0.021262 -0.051069 -0.082306  \n",
       "2      0.001704  0.098474 -0.053931 -0.007621  0.002607  \n",
       "3      0.115820  0.100294  0.055571  0.046550  0.041450  \n",
       "4     -0.044142  0.019417  0.026820  0.013483 -0.025351  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "10056  0.024695 -0.099009 -0.039917 -0.073046 -0.057320  \n",
       "10057  0.010160 -0.051351 -0.017208 -0.039342 -0.034791  \n",
       "10058 -0.032920  0.019858 -0.015019  0.093141 -0.025104  \n",
       "10059 -0.049368 -0.036179 -0.029942  0.039247 -0.071742  \n",
       "10060 -0.020004  0.010457  0.000264  0.019231  0.082335  \n",
       "\n",
       "[10061 rows x 302 columns]>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inroads_vocab = list()\n",
    "\n",
    "for i in range(0,len(inroads_vectors)):\n",
    "    test = inroads_vectors[i]\n",
    "    inroads_vocab.append(test[0])\n",
    "\n",
    "inroads_vocab_df = pd.DataFrame(inroads_vocab, columns=[\"tokens\"])\n",
    "\n",
    "inroads_vectors_2 = list()\n",
    "\n",
    "for i in range(0,len(inroads_vectors)):\n",
    "    test = inroads_vectors[i]\n",
    "    inroads_vectors_2.append(test[1])\n",
    "    \n",
    "inroads_array = np.array(inroads_vectors_2)\n",
    "inroads_vectors_df = pd.DataFrame(inroads_vectors, columns=[\"tokens\", \"vector\"])\n",
    "\n",
    "inroads_vectors_df_2 = pd.DataFrame(inroads_vectors_df.vector.tolist())\n",
    "inroads_vectors_df_2.head\n",
    "\n",
    "inroads_result = pd.concat([inroads_vectors_df, inroads_vectors_df_2], axis=1)\n",
    "inroads_result.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'D01BB6F15FA026AC',\n",
       "  'HostId': 'mhd8ggw9ZMwsGeYLjB5E9QQhtPDQKYN0s++pQDRSeaJ0Ab9k65SbQqf0J+xXdvmj8IZucrMEwaE=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'mhd8ggw9ZMwsGeYLjB5E9QQhtPDQKYN0s++pQDRSeaJ0Ab9k65SbQqf0J+xXdvmj8IZucrMEwaE=',\n",
       "   'x-amz-request-id': 'D01BB6F15FA026AC',\n",
       "   'date': 'Sat, 05 Sep 2020 01:34:25 GMT',\n",
       "   'etag': '\"f89778b8ce47e286e8ef7291c068304c\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"f89778b8ce47e286e8ef7291c068304c\"'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "bucket = 'inroads-test-bucket1' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "inroads_result.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'data/Vectorized/inroads_vectorized.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "    \n",
    "    return kmeans_clustering, kmeans_clustering.cluster_centers_, idx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "#Closest points for each Cluster center is used to query the closest 20 points to it.\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "#Word Index is queried for each position in the above array, and added to a Dictionary.\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "#A DataFrame is generated from the dictionary.\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "    return df, tree;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'old_vectors_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7d36939f155f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclustering_on_wordvecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_vectors_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcentroid_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'old_vectors_2' is not defined"
     ]
    }
   ],
   "source": [
    "kmodel,centers,clusters = clustering_on_wordvecs(old_vectors_2, 9);\n",
    "centroid_map = dict(zip(old_vocab, clusters));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_cluster = {}\n",
    "\n",
    "for i in range(0, len(inroads_vectors_2)):\n",
    "    word = inroads_vocab[i]\n",
    "    temp = kmodel.predict(inroads_vectors_2[i].reshape(1,-1))[0]\n",
    "    predicted_cluster.update({word:temp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'research': 0,\n",
       " 'article': 0,\n",
       " 'personality': 0,\n",
       " 'social': 5,\n",
       " 'psychology': 0,\n",
       " 'benevolent': 5,\n",
       " 'sexism': 6,\n",
       " 'attitudes': 0,\n",
       " 'toward': 5,\n",
       " 'bulletin': 4,\n",
       " 'society': 5,\n",
       " 'motherhood': 0,\n",
       " 'reproductive': 6,\n",
       " 'right': 5,\n",
       " 'reprint': 0,\n",
       " 'multi': 0,\n",
       " 'study': 0,\n",
       " 'longitudinal': 1,\n",
       " 'examination': 0,\n",
       " 'abortion': 6,\n",
       " 'huang': 7,\n",
       " 'paul': 3,\n",
       " 'davies': 3,\n",
       " 'chris': 7,\n",
       " 'sibley': 4,\n",
       " 'danny': 3,\n",
       " 'osborne': 2,\n",
       " 'abstract': 0,\n",
       " 'although': 8,\n",
       " 'ideology': 5,\n",
       " 'that': 2,\n",
       " 'highly': 0,\n",
       " 'revere': 3,\n",
       " 'women': 6,\n",
       " 'conform': 2,\n",
       " 'traditional': 0,\n",
       " 'gender': 0,\n",
       " 'cloak': 7,\n",
       " 'superficially': 2,\n",
       " 'positive': 2,\n",
       " 'tone': 0,\n",
       " 'place': 0,\n",
       " 'upon': 2,\n",
       " 'pedestal': 7,\n",
       " 'inherently': 5,\n",
       " 'restrictive': 6,\n",
       " 'accordingly': 0,\n",
       " 'because': 2,\n",
       " 'paternalistic': 6,\n",
       " 'beliefs': 2,\n",
       " 'associate': 0,\n",
       " 'with': 2,\n",
       " 'base': 5,\n",
       " 'idealization': 2,\n",
       " 'roles': 0,\n",
       " 'which': 0,\n",
       " 'include': 0,\n",
       " 'should': 2,\n",
       " 'predict': 2,\n",
       " 'people': 2,\n",
       " 'data': 1,\n",
       " 'from': 8,\n",
       " 'nationwide': 5,\n",
       " 'panel': 0,\n",
       " 'show': 0,\n",
       " 'hostile': 5,\n",
       " 'have': 2,\n",
       " 'cross': 7,\n",
       " 'effect': 6,\n",
       " 'opposition': 5,\n",
       " 'both': 0,\n",
       " 'elective': 1,\n",
       " 'traumatic': 2,\n",
       " 'extend': 0,\n",
       " 'these': 0,\n",
       " 'find': 2,\n",
       " 'relationship': 0,\n",
       " 'between': 8,\n",
       " 'support': 5,\n",
       " 'fully': 0,\n",
       " 'mediate': 0,\n",
       " 'result': 5,\n",
       " 'highlight': 0,\n",
       " 'pernicious': 6,\n",
       " 'nature': 0,\n",
       " 'demonstrate': 0,\n",
       " 'substantial': 5,\n",
       " 'cost': 1,\n",
       " 'namely': 0,\n",
       " 'restriction': 5,\n",
       " 'keywords': 0,\n",
       " 'receive': 1,\n",
       " 'revision': 0,\n",
       " 'accept': 2,\n",
       " 'april': 4,\n",
       " 'often': 2,\n",
       " 'regard': 0,\n",
       " 'essential': 0,\n",
       " 'component': 0,\n",
       " 'woman': 3,\n",
       " 'health': 1,\n",
       " 'life': 2,\n",
       " 'fisher': 3,\n",
       " 'rowe': 3,\n",
       " 'would': 2,\n",
       " 'compromise': 5,\n",
       " 'bring': 0,\n",
       " 'pregnancy': 1,\n",
       " 'term': 0,\n",
       " 'indeed': 2,\n",
       " 'become': 8,\n",
       " 'mother': 3,\n",
       " 'many': 2,\n",
       " 'rape': 6,\n",
       " 'notably': 8,\n",
       " 'highest': 5,\n",
       " 'role': 5,\n",
       " 'complete': 1,\n",
       " 'higher': 5,\n",
       " 'than': 2,\n",
       " 'gorman': 3,\n",
       " 'johnston': 3,\n",
       " 'view': 0,\n",
       " 'marcos': 8,\n",
       " 'craig': 3,\n",
       " 'decide': 4,\n",
       " 'least': 2,\n",
       " 'temporarily': 7,\n",
       " 'reject': 2,\n",
       " 'this': 0,\n",
       " 'sacred': 2,\n",
       " 'choose': 2,\n",
       " 'terminate': 1,\n",
       " 'current': 0,\n",
       " 'contribute': 0,\n",
       " 'debate': 5,\n",
       " 'directly': 5,\n",
       " 'meet': 4,\n",
       " 'criticism': 0,\n",
       " 'examine': 0,\n",
       " 'impact': 0,\n",
       " 'observations': 0,\n",
       " 'such': 0,\n",
       " 'lead': 5,\n",
       " 'cook': 3,\n",
       " 'jelen': 8,\n",
       " 'wilcox': 0,\n",
       " 'their': 2,\n",
       " 'within': 0,\n",
       " 'contend': 5,\n",
       " 'general': 5,\n",
       " 'follow': 0,\n",
       " 'sectional': 5,\n",
       " 'public': 5,\n",
       " 'partially': 2,\n",
       " 'about': 2,\n",
       " 'appropriate': 0,\n",
       " 'before': 4,\n",
       " 'describe': 0,\n",
       " 'stan': 3,\n",
       " 'short': 7,\n",
       " 'function': 0,\n",
       " 'socio': 0,\n",
       " 'demographic': 8,\n",
       " 'correlate': 0,\n",
       " 'restrict': 5,\n",
       " 'choices': 5,\n",
       " 'then': 7,\n",
       " 'introduce': 0,\n",
       " 'literature': 0,\n",
       " 'ambivalent': 0,\n",
       " 'theory': 0,\n",
       " 'when': 2,\n",
       " 'discuss': 0,\n",
       " 'over': 8,\n",
       " 'fiske': 3,\n",
       " 'reconcile': 5,\n",
       " 'some': 2,\n",
       " 'easy': 2,\n",
       " 'rely': 5,\n",
       " 'common': 0,\n",
       " 'nomenclature': 0,\n",
       " 'cies': 3,\n",
       " 'identify': 0,\n",
       " 'past': 8,\n",
       " 'conclude': 0,\n",
       " 'section': 0,\n",
       " 'choice': 5,\n",
       " 'imply': 0,\n",
       " 'relatively': 8,\n",
       " 'uniform': 7,\n",
       " 'university': 4,\n",
       " 'auckland': 3,\n",
       " 'zealand': 6,\n",
       " 'ever': 2,\n",
       " 'vary': 0,\n",
       " 'circumstances': 2,\n",
       " 'british': 8,\n",
       " 'columbia': 4,\n",
       " 'canada': 8,\n",
       " 'surround': 0,\n",
       " 'decision': 5,\n",
       " 'correspond': 0,\n",
       " 'author': 0,\n",
       " 'alvarez': 3,\n",
       " 'brehm': 2,\n",
       " 'kane': 3,\n",
       " 'martinez': 3,\n",
       " 'school': 3,\n",
       " 'private': 5,\n",
       " 'those': 2,\n",
       " 'financial': 5,\n",
       " 'insecurity': 6,\n",
       " 'want': 2,\n",
       " 'child': 3,\n",
       " 'email': 7,\n",
       " 'program': 5,\n",
       " 'hypotheses': 0,\n",
       " 'help': 2,\n",
       " 'establish': 5,\n",
       " 'direction': 5,\n",
       " 'maintenance': 5,\n",
       " 'process': 5,\n",
       " 'through': 0,\n",
       " 'relationships': 0,\n",
       " 'occur': 8,\n",
       " 'critical': 0,\n",
       " 'predictors': 5,\n",
       " 'turn': 2,\n",
       " 'review': 0,\n",
       " 'position': 5,\n",
       " 'argue': 0,\n",
       " 'subordination': 6,\n",
       " 'maintain': 5,\n",
       " 'complementary': 0,\n",
       " 'ideologies': 0,\n",
       " 'whereas': 8,\n",
       " 'punish': 6,\n",
       " 'norm': 5,\n",
       " 'violate': 5,\n",
       " 'root': 8,\n",
       " 'idealize': 0,\n",
       " 'numerous': 0,\n",
       " 'attitudinal': 5,\n",
       " 'variables': 5,\n",
       " 'perhaps': 2,\n",
       " 'thus': 5,\n",
       " 'constrain': 5,\n",
       " 'praise': 2,\n",
       " 'indicate': 0,\n",
       " 'church': 8,\n",
       " 'attendance': 4,\n",
       " 'ellison': 3,\n",
       " 'adhere': 5,\n",
       " 'while': 2,\n",
       " 'devalue': 0,\n",
       " 'smith': 3,\n",
       " 'prescriptions': 2,\n",
       " 'religiosity': 8,\n",
       " 'bailey': 3,\n",
       " 'werner': 3,\n",
       " 'nevertheless': 2,\n",
       " 'benin': 8,\n",
       " 'esposito': 3,\n",
       " 'escape': 2,\n",
       " 'negatively': 6,\n",
       " 'label': 0,\n",
       " 'likewise': 0,\n",
       " 'political': 5,\n",
       " 'consistently': 5,\n",
       " 'emerge': 8,\n",
       " 'covert': 5,\n",
       " 'form': 0,\n",
       " 'facilitate': 5,\n",
       " 'specifically': 0,\n",
       " 'conservatism': 8,\n",
       " 'inequalities': 6,\n",
       " 'oftentimes': 2,\n",
       " 'little': 2,\n",
       " 'party': 5,\n",
       " 'identification': 0,\n",
       " 'oppress': 2,\n",
       " 'also': 0,\n",
       " 'jackman': 7,\n",
       " 'where': 2,\n",
       " 'hess': 3,\n",
       " 'pant': 7,\n",
       " 'evaluate': 0,\n",
       " 'passages': 0,\n",
       " 'contain': 0,\n",
       " 'either': 2,\n",
       " 'negative': 2,\n",
       " 'barreto': 3,\n",
       " 'more': 2,\n",
       " 'prejudice': 2,\n",
       " 'felt': 2,\n",
       " 'greater': 5,\n",
       " 'anger': 2,\n",
       " 'across': 8,\n",
       " 'culture': 8,\n",
       " 'passage': 6,\n",
       " 'less': 2,\n",
       " 'sage': 3,\n",
       " 'another': 2,\n",
       " 'despite': 5,\n",
       " 'consistencies': 0,\n",
       " 'encounter': 0,\n",
       " 'increase': 5,\n",
       " 'religious': 5,\n",
       " 'endorsement': 5,\n",
       " 'fischer': 3,\n",
       " 'differences': 5,\n",
       " 'together': 0,\n",
       " 'yield': 2,\n",
       " 'inconsistent': 2,\n",
       " 'sometimes': 2,\n",
       " 'system': 5,\n",
       " 'supportive': 5,\n",
       " 'polarize': 5,\n",
       " 'patel': 2,\n",
       " 'johns': 3,\n",
       " 'admiration': 2,\n",
       " 'respectively': 1,\n",
       " 'level': 5,\n",
       " 'tendency': 2,\n",
       " 'finlay': 4,\n",
       " 'note': 0,\n",
       " 'however': 2,\n",
       " 'though': 2,\n",
       " 'becker': 0,\n",
       " 'tend': 2,\n",
       " 'most': 0,\n",
       " 'likely': 2,\n",
       " 'endorse': 5,\n",
       " 'case': 6,\n",
       " 'response': 5,\n",
       " 'hold': 5,\n",
       " 'termination': 1,\n",
       " 'seek': 5,\n",
       " 'specific': 0,\n",
       " 'incongruent': 2,\n",
       " 'reason': 2,\n",
       " 'reaction': 2,\n",
       " 'complex': 0,\n",
       " 'appear': 0,\n",
       " 'corroborate': 0,\n",
       " 'first': 0,\n",
       " 'blush': 2,\n",
       " 'wilson': 3,\n",
       " 'expose': 2,\n",
       " 'better': 2,\n",
       " 'itself': 8,\n",
       " 'express': 2,\n",
       " 'lower': 6,\n",
       " 'wall': 7,\n",
       " 'colleagues': 3,\n",
       " 'ment': 1,\n",
       " 'other': 2,\n",
       " 'word': 2,\n",
       " 'encourage': 5,\n",
       " 'tion': 1,\n",
       " 'into': 8,\n",
       " 'saint': 3,\n",
       " 'sinners': 2,\n",
       " 'sanctity': 5,\n",
       " 'finally': 2,\n",
       " 'anoint': 3,\n",
       " 'like': 7,\n",
       " 'status': 5,\n",
       " 'adherence': 5,\n",
       " 'egalitarian': 5,\n",
       " 'positively': 2,\n",
       " 'port': 4,\n",
       " 'autonomy': 5,\n",
       " 'warm': 7,\n",
       " 'attribute': 2,\n",
       " 'unique': 0,\n",
       " 'association': 5,\n",
       " 'counterparts': 8,\n",
       " 'reproduction': 0,\n",
       " 'legalize': 6,\n",
       " 'pregnant': 1,\n",
       " 'young': 3,\n",
       " 'infants': 1,\n",
       " 'well': 0,\n",
       " 'plausible': 2,\n",
       " 'mechanism': 5,\n",
       " 'respect': 5,\n",
       " 'behind': 7,\n",
       " 'fill': 7,\n",
       " 'post': 8,\n",
       " 'menopausal': 1,\n",
       " 'critically': 0,\n",
       " 'important': 0,\n",
       " 'favorable': 5,\n",
       " 'evaluations': 5,\n",
       " 'solely': 5,\n",
       " 'assess': 0,\n",
       " 'pressure': 5,\n",
       " 'careful': 0,\n",
       " 'monitor': 1,\n",
       " 'date': 0,\n",
       " 'rule': 5,\n",
       " 'possibility': 2,\n",
       " 'behaviors': 6,\n",
       " 'influence': 5,\n",
       " 'address': 0,\n",
       " 'strict': 6,\n",
       " 'prescriptive': 0,\n",
       " 'during': 8,\n",
       " 'limitation': 5,\n",
       " 'hostility': 2,\n",
       " 'conduct': 1,\n",
       " 'king': 4,\n",
       " 'dress': 7,\n",
       " 'engage': 0,\n",
       " 'test': 1,\n",
       " 'retail': 7,\n",
       " 'staff': 1,\n",
       " 'shop': 7,\n",
       " 'customer': 7,\n",
       " 'confederate': 3,\n",
       " 'report': 1,\n",
       " 'stable': 5,\n",
       " 'resistant': 5,\n",
       " 'change': 5,\n",
       " 'benevolence': 2,\n",
       " 'gillespie': 3,\n",
       " 'federate': 5,\n",
       " 'pose': 0,\n",
       " 'customers': 7,\n",
       " 'probable': 2,\n",
       " 'potential': 5,\n",
       " 'candidates': 5,\n",
       " 'masculine': 0,\n",
       " 'applicants': 1,\n",
       " 'relative': 8,\n",
       " 'independently': 5,\n",
       " 'regulatory': 5,\n",
       " 'high': 6,\n",
       " 'assumption': 0,\n",
       " 'make': 2,\n",
       " 'approve': 4,\n",
       " 'breastfeed': 1,\n",
       " 'novel': 0,\n",
       " 'contribution': 0,\n",
       " 'strongly': 5,\n",
       " 'acker': 0,\n",
       " 'sutton': 3,\n",
       " 'subsequently': 1,\n",
       " 'provide': 0,\n",
       " 'comprehensive': 0,\n",
       " 'murphy': 3,\n",
       " 'douglas': 3,\n",
       " 'mcclellan': 3,\n",
       " 'assessment': 0,\n",
       " 'uniquely': 0,\n",
       " 'associations': 5,\n",
       " 'explain': 0,\n",
       " 'fail': 5,\n",
       " 'satisfy': 2,\n",
       " 'ideals': 5,\n",
       " 'overview': 0,\n",
       " 'give': 2,\n",
       " 'various': 0,\n",
       " 'previous': 0,\n",
       " 'investigate': 0,\n",
       " 'relation': 0,\n",
       " 'ship': 7,\n",
       " 'begin': 8,\n",
       " 'absent': 0,\n",
       " 'type': 0,\n",
       " 'inde': 8,\n",
       " 'oversight': 5,\n",
       " 'abor': 1,\n",
       " 'only': 2,\n",
       " 'year': 4,\n",
       " 'later': 3,\n",
       " 'robustness': 1,\n",
       " 'orientation': 5,\n",
       " 'nationally': 5,\n",
       " 'representative': 5,\n",
       " 'equality': 5,\n",
       " 'much': 2,\n",
       " 'need': 2,\n",
       " 'particularly': 0,\n",
       " 'predictor': 2,\n",
       " 'hypothesis': 0,\n",
       " 'definition': 0,\n",
       " 'generalize': 2,\n",
       " 'belief': 2,\n",
       " 'systems': 5,\n",
       " 'guide': 0,\n",
       " 'policy': 5,\n",
       " 'preferences': 5,\n",
       " 'build': 5,\n",
       " 'nascent': 0,\n",
       " 'conway': 3,\n",
       " 'lend': 2,\n",
       " 'residual': 2,\n",
       " 'period': 8,\n",
       " 'responses': 0,\n",
       " 'participants': 0,\n",
       " 'retain': 5,\n",
       " 'could': 2,\n",
       " 'rejection': 5,\n",
       " 'wave': 8,\n",
       " 'copy': 7,\n",
       " 'questionnaire': 1,\n",
       " 'reminder': 2,\n",
       " 'postal': 1,\n",
       " 'send': 4,\n",
       " 'months': 4,\n",
       " 'invite': 0,\n",
       " 'identical': 2,\n",
       " 'online': 0,\n",
       " 'version': 0,\n",
       " 'compensate': 2,\n",
       " 'attrition': 5,\n",
       " 'booster': 7,\n",
       " 'sample': 1,\n",
       " 'recruit': 4,\n",
       " 'survey': 0,\n",
       " 'major': 8,\n",
       " 'framework': 0,\n",
       " 'newspaper': 4,\n",
       " 'website': 4,\n",
       " 'invitation': 4,\n",
       " 'participate': 5,\n",
       " 'respond': 5,\n",
       " 'complicate': 0,\n",
       " 'hand': 7,\n",
       " 'largely': 8,\n",
       " 'time': 8,\n",
       " 'issue': 5,\n",
       " 'prior': 5,\n",
       " 'underlie': 0,\n",
       " 'adoption': 6,\n",
       " 'additions': 0,\n",
       " 'further': 0,\n",
       " 'second': 8,\n",
       " 'point': 0,\n",
       " 'retention': 1,\n",
       " 'rate': 1,\n",
       " 'deal': 5,\n",
       " 'mentally': 6,\n",
       " 'outside': 7,\n",
       " 'control': 5,\n",
       " 'agentic': 0,\n",
       " 'might': 2,\n",
       " 'still': 2,\n",
       " 'violation': 6,\n",
       " 'ideal': 0,\n",
       " 'sacrificial': 3,\n",
       " 'analyse': 0,\n",
       " 'focus': 0,\n",
       " 'partial': 2,\n",
       " 'heart': 7,\n",
       " 'therefore': 2,\n",
       " 'relevant': 0,\n",
       " 'scale': 8,\n",
       " 'will': 2,\n",
       " 'consistent': 0,\n",
       " 'obtain': 1,\n",
       " 'ethnic': 8,\n",
       " 'breakdown': 2,\n",
       " 'prediction': 2,\n",
       " 'european': 8,\n",
       " 'identi': 3,\n",
       " 'alternative': 0,\n",
       " 'fied': 3,\n",
       " 'māori': 3,\n",
       " 'pacific': 8,\n",
       " 'nations': 8,\n",
       " 'descent': 8,\n",
       " 'asian': 8,\n",
       " 'ethnicity': 0,\n",
       " 'accord': 2,\n",
       " 'cognitive': 0,\n",
       " 'dissonance': 0,\n",
       " 'incline': 2,\n",
       " 'consistency': 0,\n",
       " 'motivate': 5,\n",
       " 'align': 5,\n",
       " 'fairly': 2,\n",
       " 'measure': 5,\n",
       " 'suggest': 0,\n",
       " 'force': 5,\n",
       " 'inventory': 0,\n",
       " 'covariates': 1,\n",
       " 'descriptive': 0,\n",
       " 'statistics': 1,\n",
       " 'bivariate': 1,\n",
       " 'correlations': 0,\n",
       " 'affect': 0,\n",
       " 'display': 7,\n",
       " 'table': 7,\n",
       " 'mean': 2,\n",
       " 'model': 0,\n",
       " 'standard': 5,\n",
       " 'deviations': 1,\n",
       " 'affirmative': 6,\n",
       " 'action': 5,\n",
       " 'therein': 0,\n",
       " 'represent': 0,\n",
       " 'score': 7,\n",
       " 'estimate': 1,\n",
       " 'latent': 0,\n",
       " 'focal': 0,\n",
       " 'method': 1,\n",
       " 'procedure': 1,\n",
       " 'come': 2,\n",
       " 'items': 7,\n",
       " 'item': 1,\n",
       " 'value': 5,\n",
       " 'example': 0,\n",
       " 'quality': 1,\n",
       " 'structural': 5,\n",
       " 'equation': 0,\n",
       " 'variable': 0,\n",
       " 'contrast': 0,\n",
       " 'range': 0,\n",
       " 'extremely': 2,\n",
       " 'liberal': 5,\n",
       " 'conservative': 5,\n",
       " 'oppose': 5,\n",
       " 'disagree': 2,\n",
       " 'agree': 2,\n",
       " 'purity': 2,\n",
       " 'possess': 2,\n",
       " 'enders': 3,\n",
       " 'gain': 5,\n",
       " 'power': 5,\n",
       " 'reliable': 1,\n",
       " 'deletion': 1,\n",
       " 'avail': 2,\n",
       " 'anchor': 0,\n",
       " 'able': 2,\n",
       " 'importantly': 5,\n",
       " 'inflate': 2,\n",
       " 'errors': 1,\n",
       " 'require': 5,\n",
       " 'miss': 3,\n",
       " 'completely': 2,\n",
       " 'random': 1,\n",
       " 'prefer': 2,\n",
       " 'estimation': 1,\n",
       " 'unavoidable': 2,\n",
       " 'consequence': 2,\n",
       " 'adapt': 0,\n",
       " 'description': 0,\n",
       " 'hypothesize': 0,\n",
       " 'marsden': 8,\n",
       " 'hout': 8,\n",
       " 'specify': 1,\n",
       " 'regardless': 5,\n",
       " 'baseline': 1,\n",
       " 'endanger': 2,\n",
       " 'reassess': 0,\n",
       " 'policies': 5,\n",
       " 'male': 0,\n",
       " 'female': 0,\n",
       " 'account': 0,\n",
       " 'each': 0,\n",
       " 'allow': 5,\n",
       " 'recommend': 1,\n",
       " 'square': 4,\n",
       " 'error': 2,\n",
       " 'approximation': 0,\n",
       " 'standardize': 1,\n",
       " 'excellent': 0,\n",
       " 'enhance': 5,\n",
       " 'sustainable': 5,\n",
       " 'comparative': 0,\n",
       " 'business': 5,\n",
       " 'growth': 8,\n",
       " 'among': 0,\n",
       " 'businesses': 5,\n",
       " 'operate': 5,\n",
       " 'index': 0,\n",
       " 'incentives': 5,\n",
       " 'figure': 0,\n",
       " 'overall': 0,\n",
       " 'workforce': 5,\n",
       " 'government': 5,\n",
       " 'variance': 5,\n",
       " 'inspection': 7,\n",
       " 'individual': 5,\n",
       " 'pathways': 0,\n",
       " 'confidence': 2,\n",
       " 'interval': 1,\n",
       " 'purpose': 5,\n",
       " 'exert': 5,\n",
       " 'promotion': 5,\n",
       " 'echo': 0,\n",
       " 'particular': 0,\n",
       " 'reverence': 2,\n",
       " 'significant': 0,\n",
       " 'adjust': 1,\n",
       " 'same': 2,\n",
       " 'frame': 0,\n",
       " 'even': 2,\n",
       " 'after': 4,\n",
       " 'reciprocal': 0,\n",
       " 'utilize': 0,\n",
       " 'others': 2,\n",
       " 'ensure': 5,\n",
       " 'comparable': 8,\n",
       " 'full': 2,\n",
       " 'information': 1,\n",
       " 'maximum': 1,\n",
       " 'likelihood': 6,\n",
       " 'possible': 2,\n",
       " 'once': 2,\n",
       " 'again': 7,\n",
       " 'coefficients': 1,\n",
       " 'similarly': 6,\n",
       " 'precede': 0,\n",
       " 'addition': 0,\n",
       " 'produce': 0,\n",
       " 'definitive': 0,\n",
       " 'rather': 2,\n",
       " 'vice': 5,\n",
       " 'versa': 5,\n",
       " 'matrix': 0,\n",
       " 'render': 2,\n",
       " 'interpretation': 0,\n",
       " 'path': 8,\n",
       " 'ways': 0,\n",
       " 'problematic': 0,\n",
       " 'poor': 6,\n",
       " 'discussion': 0,\n",
       " 'entire': 8,\n",
       " 'hare': 4,\n",
       " 'never': 2,\n",
       " 'moreover': 5,\n",
       " 'problems': 5,\n",
       " 'unlike': 8,\n",
       " 'pursue': 5,\n",
       " 'encompass': 0,\n",
       " 'situations': 2,\n",
       " 'resentment': 2,\n",
       " 'experience': 0,\n",
       " 'confront': 2,\n",
       " 'demonstration': 4,\n",
       " 'basis': 5,\n",
       " 'certain': 2,\n",
       " 'aspects': 0,\n",
       " 'explanatory': 0,\n",
       " 'responsible': 5,\n",
       " 'remain': 8,\n",
       " 'unknown': 2,\n",
       " 'importance': 0,\n",
       " 'under': 5,\n",
       " 'pathway': 0,\n",
       " 'thesis': 0,\n",
       " 'ambivalently': 8,\n",
       " 'sexist': 6,\n",
       " 'censure': 2,\n",
       " 'harmful': 6,\n",
       " 'venerate': 0,\n",
       " 'fetus': 1,\n",
       " 'societal': 5,\n",
       " 'undergraduate': 0,\n",
       " 'nurture': 2,\n",
       " 'caregivers': 1,\n",
       " 'long': 8,\n",
       " 'students': 0,\n",
       " 'years': 4,\n",
       " 'fertility': 1,\n",
       " 'part': 0,\n",
       " 'course': 0,\n",
       " 'broderick': 3,\n",
       " 'requirements': 5,\n",
       " 'posit': 0,\n",
       " 'stereotype': 6,\n",
       " 'marry': 3,\n",
       " 'islander': 4,\n",
       " 'family': 3,\n",
       " 'plan': 1,\n",
       " 'division': 5,\n",
       " 'labor': 5,\n",
       " 'households': 6,\n",
       " 'bennett': 3,\n",
       " 'whether': 2,\n",
       " 'hood': 7,\n",
       " 'personal': 0,\n",
       " 'critique': 0,\n",
       " 'relate': 0,\n",
       " 'main': 0,\n",
       " 'driver': 7,\n",
       " 'naturally': 2,\n",
       " 'maternal': 1,\n",
       " 'seven': 4,\n",
       " 'conception': 0,\n",
       " 'three': 8,\n",
       " 'seriously': 2,\n",
       " 'nancy': 3,\n",
       " 'four': 4,\n",
       " 'modifications': 1,\n",
       " 'capitalize': 0,\n",
       " 'chance': 2,\n",
       " 'subscale': 2,\n",
       " 'real': 2,\n",
       " 'until': 4,\n",
       " 'doesn': 7,\n",
       " 'children': 3,\n",
       " 'unnatural': 2,\n",
       " 'direct': 5,\n",
       " 'indirect': 6,\n",
       " 'factor': 5,\n",
       " 'aforementioned': 0,\n",
       " 'load': 7,\n",
       " 'zero': 7,\n",
       " 'link': 0,\n",
       " 'mediation': 0,\n",
       " 'bias': 6,\n",
       " 'predictions': 2,\n",
       " 'correct': 2,\n",
       " 'preacher': 3,\n",
       " 'bootstrap': 2,\n",
       " 'good': 2,\n",
       " 'below': 7,\n",
       " 'cutoff': 4,\n",
       " 'reflect': 0,\n",
       " 'parentheses': 3,\n",
       " 'indicator': 1,\n",
       " 'exclude': 5,\n",
       " 'presentation': 0,\n",
       " 'unexpectedly': 7,\n",
       " 'theorize': 0,\n",
       " 'mediator': 5,\n",
       " 'signifi': 2,\n",
       " 'cant': 2,\n",
       " 'size': 7,\n",
       " 'slightly': 7,\n",
       " 'stronger': 5,\n",
       " 'aspect': 0,\n",
       " 'larger': 0,\n",
       " 'remember': 3,\n",
       " 'correlational': 1,\n",
       " 'there': 2,\n",
       " 'prescribe': 1,\n",
       " 'unlikely': 2,\n",
       " 'scenario': 2,\n",
       " 'ideological': 5,\n",
       " 'downstream': 1,\n",
       " 'consequences': 6,\n",
       " 'distal': 1,\n",
       " 'assumptions': 0,\n",
       " 'notion': 0,\n",
       " 'work': 0,\n",
       " 'create': 5,\n",
       " 'gendered': 0,\n",
       " 'expectations': 0,\n",
       " 'fact': 2,\n",
       " 'demon': 7,\n",
       " 'play': 0,\n",
       " 'tandem': 8,\n",
       " 'they': 2,\n",
       " 'extension': 5,\n",
       " 'covertly': 6,\n",
       " 'live': 2,\n",
       " 'detrimental': 6,\n",
       " 'outcomes': 5,\n",
       " 'broader': 0,\n",
       " 'insights': 0,\n",
       " 'perception': 2,\n",
       " 'moth': 7,\n",
       " 'inoculate': 7,\n",
       " 'feel': 2,\n",
       " 'brunt': 8,\n",
       " 'subordinate': 5,\n",
       " 'attenuate': 2,\n",
       " 'dominance': 5,\n",
       " 'fraser': 5,\n",
       " 'apparent': 2,\n",
       " 'benefit': 5,\n",
       " 'domestic': 6,\n",
       " 'undermine': 5,\n",
       " 'indirectly': 5,\n",
       " 'implications': 0,\n",
       " 'limitations': 0,\n",
       " 'future': 2,\n",
       " 'directions': 0,\n",
       " 'marginally': 6,\n",
       " 'retrospect': 2,\n",
       " 'unexpected': 0,\n",
       " 'strong': 5,\n",
       " 'evidence': 6,\n",
       " 'assertion': 2,\n",
       " 'reward': 2,\n",
       " 'deviate': 2,\n",
       " 'speak': 2,\n",
       " 'career': 3,\n",
       " 'home': 3,\n",
       " 'offer': 0,\n",
       " 'strongest': 5,\n",
       " 'duties': 5,\n",
       " 'believe': 2,\n",
       " 'replicate': 0,\n",
       " 'derive': 0,\n",
       " 'understand': 0,\n",
       " 'student': 3,\n",
       " 'third': 8,\n",
       " 'unmeasured': 2,\n",
       " 'caveat': 2,\n",
       " 'worth': 2,\n",
       " 'design': 5,\n",
       " 'causal': 0,\n",
       " 'inform': 0,\n",
       " 'modification': 1,\n",
       " 'indices': 0,\n",
       " 'drive': 7,\n",
       " 'trajectory': 8,\n",
       " 'inappropriate': 2,\n",
       " 'pattern': 0,\n",
       " 'theoretically': 0,\n",
       " 'curve': 7,\n",
       " 'meredith': 4,\n",
       " 'dent': 2,\n",
       " 'construct': 0,\n",
       " 'look': 0,\n",
       " 'forward': 0,\n",
       " 'concern': 0,\n",
       " 'rationale': 5,\n",
       " 'parameter': 1,\n",
       " 'measurement': 1,\n",
       " 'lack': 2,\n",
       " 'space': 0,\n",
       " 'constraints': 5,\n",
       " 'single': 0,\n",
       " 'reliability': 1,\n",
       " 'lieu': 6,\n",
       " 'cations': 3,\n",
       " 'caution': 2,\n",
       " 'inter': 8,\n",
       " 'outcome': 5,\n",
       " 'light': 0,\n",
       " 'difficult': 2,\n",
       " 'converge': 8,\n",
       " 'antecedent': 8,\n",
       " 'workplace': 6,\n",
       " 'antipathy': 8,\n",
       " 'interpret': 0,\n",
       " 'employment': 6,\n",
       " 'necessity': 2,\n",
       " 'russo': 3,\n",
       " 'decisions': 5,\n",
       " 'myth': 0,\n",
       " 'younger': 3,\n",
       " 'mediators': 5,\n",
       " 'parenthood': 1,\n",
       " 'stance': 5,\n",
       " 'them': 2,\n",
       " 'perceive': 2,\n",
       " 'manner': 2,\n",
       " 'older': 3,\n",
       " 'population': 8,\n",
       " 'conclusion': 0,\n",
       " 'large': 5,\n",
       " 'omnibus': 4,\n",
       " 'unable': 2,\n",
       " 'crucial': 0,\n",
       " 'overcome': 2,\n",
       " 'additional': 1,\n",
       " 'ultimately': 5,\n",
       " 'persistence': 5,\n",
       " 'insidious': 2,\n",
       " 'subjectively': 2,\n",
       " 'reference': 0,\n",
       " 'breast': 7,\n",
       " 'best': 2,\n",
       " 'everywhere': 2,\n",
       " 'promote': 5,\n",
       " 'towards': 8,\n",
       " 'analysis': 0,\n",
       " 'urban': 8,\n",
       " 'indicators': 5,\n",
       " 'acknowledgments': 3,\n",
       " 'adolescents': 6,\n",
       " 'master': 3,\n",
       " 'italy': 8,\n",
       " 'sweden': 8,\n",
       " 'science': 0,\n",
       " 'supervise': 4,\n",
       " 'medicine': 1,\n",
       " 'stability': 5,\n",
       " 'declaration': 5,\n",
       " 'conflict': 5,\n",
       " 'interest': 5,\n",
       " 'declare': 4,\n",
       " 'multidisciplinary': 0,\n",
       " 'authorship': 0,\n",
       " 'publication': 0,\n",
       " 'journal': 0,\n",
       " 'american': 8,\n",
       " 'ambivalence': 2,\n",
       " 'fund': 5,\n",
       " 'development': 5,\n",
       " ...}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '67E2A2E7A67EDC17',\n",
       "  'HostId': 'Ri+AT+SvNKg3pC06aHSiE6p26ULoqaFWQUU2qKUYcuXOvcP2g3ZQnZgNnegsvReaJcxVxbG4o08=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'Ri+AT+SvNKg3pC06aHSiE6p26ULoqaFWQUU2qKUYcuXOvcP2g3ZQnZgNnegsvReaJcxVxbG4o08=',\n",
       "   'x-amz-request-id': '67E2A2E7A67EDC17',\n",
       "   'date': 'Tue, 18 Aug 2020 23:44:18 GMT',\n",
       "   'etag': '\"541bbc9f1e752eaa261d9ac22a639f57\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"541bbc9f1e752eaa261d9ac22a639f57\"'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "serializedclusters = pickle.dumps(predicted_cluster)\n",
    "s3.put_object(Bucket='inroads-test-bucket1',Key='myDictionary', Body=serializedclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "object = s3.get_object(Bucket='inroads-test-bucket1',Key='myDictionary')\n",
    "serializedObject = object['Body'].read()\n",
    "myData = pickle.loads(serializedObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_map_df = pd.DataFrame(list(centroid_map.items()),columns = ['token','cluster']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import saved cluster structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.load(\"cluster_9_centers.npy\")\n",
    "clusters = np.load(\"cluster_9_clusters.npy\")\n",
    "centroid_map = pd.read_csv(\"cluster_9.csv\", header=None)\n",
    "centroid_map.columns = [\"token\", \"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Cluster #0     Cluster #1  Cluster #2   Cluster #3 Cluster #4  \\\n",
      "1    narrative    misoprostol        feel         wife       june   \n",
      "2   literature   mifepristone        fear       father    october   \n",
      "3     identity       surgical        harm      husband   december   \n",
      "4    knowledge      pregnancy        what     daughter      april   \n",
      "5    sexuality  complications        pain         aunt       july   \n",
      "6     cultural        medical       shame    boyfriend     august   \n",
      "7       ethics     dilatation       grief  grandmother   february   \n",
      "8      concept        uterine       wrong      brother    morning   \n",
      "9   discussion     ultrasound     disgust       sister    january   \n",
      "10  complexity       referral        love       cousin      march   \n",
      "11    feminism       cervical        evil         girl   saturday   \n",
      "12    research  contraceptive        good       friend       week   \n",
      "13    dialogue      antenatal       moral         baby      night   \n",
      "14      gender       delivery     morally       mother  afternoon   \n",
      "15        book         uterus      person        marry  september   \n",
      "16    feminist      trimester      suffer   girlfriend   thursday   \n",
      "17     content       clinical      sorrow        woman     summer   \n",
      "18     complex      gestation  compassion       auntie   november   \n",
      "19  motherhood    antibiotics        pity  grandfather      month   \n",
      "20        work        surgery   emotional        niece    tuesday   \n",
      "\n",
      "       Cluster #5       Cluster #6 Cluster #7      Cluster #8  \n",
      "1      government         violence       legs          europe  \n",
      "2          policy             rape       feet    christianity  \n",
      "3           right  criminalization     yellow           world  \n",
      "4       political            abuse   sidewalk           islam  \n",
      "5         support            crime      sleep          africa  \n",
      "6           power   discrimination    stomach            asia  \n",
      "7          action         abortion      bleed          period  \n",
      "8        equality           sexual     toilet     colonialism  \n",
      "9   organizations           stigma      stick        religion  \n",
      "10          state    incarceration    abdomen       communism  \n",
      "11     commitment       punishment       bike       caribbean  \n",
      "12     regulation     prostitution    scissor         america  \n",
      "13         system             laws     purple  fundamentalism  \n",
      "14     opposition         criminal    bicycle        buddhism  \n",
      "15    legislation      infanticide    garbage       centuries  \n",
      "16        process   stigmatization      night           latin  \n",
      "17       autonomy             drug     breath      capitalism  \n",
      "18           laws          illegal       boat           south  \n",
      "19         social          poverty      shoot      revolution  \n",
      "20           rule            women       room     catholicism  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "top_words,tree = get_top_words(old_vocab, 20, centers, old_array)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_array[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = old_vectors[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 1.22595241, 1.23703717]]), array([[  1, 797, 222]]))"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.query(old_vectors[1][1].reshape(1,-1), k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "tree = KMeans(9).fit(old_vectors[1][1])\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge vectors with clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token       object\n",
       "vector      object\n",
       "0          float64\n",
       "1          float64\n",
       "2          float64\n",
       "            ...   \n",
       "296        float64\n",
       "297        float64\n",
       "298        float64\n",
       "299        float64\n",
       "cluster     object\n",
       "Length: 303, dtype: object"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.merge(result, centroid_map, how='left', on=['token'])\n",
    "training_data = pd.merge(old_vocab_df, centroid_map, how='left', on=['token'])\n",
    "training_data.to_csv('/home/ec2-user/SageMaker/deploy/training_data.csv',index=False)\n",
    "\n",
    "result['cluster'] = result.cluster.astype(str)\n",
    "result.dtypes \n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "bucket = 'sagemaker-word2vec-scikitlearn'\n",
    "region = 'us-east-2'\n",
    "s3_session = boto3.Session().resource('s3')\n",
    "s3_session.Bucket(bucket).Object('train/train.csv').upload_file('cluster_9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0253bc4b06ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://sagemaker-word2vec-scikitlearn.s3.us-east-2.amazonaws.com/train/train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_location = 'https://sagemaker-word2vec-scikitlearn.s3.us-east-2.amazonaws.com/train/train.csv'\n",
    "\n",
    "train_data = pd.read_csv(data_location, header=None, names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.dropna()\n",
    "y = result.cluster\n",
    "X = result.iloc[:, 2:302]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6779661016949152"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "\n",
    "clf.predict_proba(X_test)\n",
    "\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "lin_clf = svm.LinearSVC()\n",
    "clf = lin_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.94915254237289"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)*100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given the two relative accuracies it appears that logistic regression is just as powerful as svm, thus we can continue with logistic regression for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
